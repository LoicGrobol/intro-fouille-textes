{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faire des classifieurs avec `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce cours, aucune compétence en programmation n'est nécessaire, et tout peut se faire avec weka et le script de vectorisation fourni. Cependant, en dehors de ce cours, il est souvent utile de savoir utiliser un langage de script comme Python pour faire de la classification de documents par apprentissage artificiel.\n",
    "\n",
    "Celà permet par exemple de modifier et d'étendre facilement les prétraitements, et de réaliser ces derniers et l'apprentissage et l'évaluation de classifieurs dans le même environnement. Cela facilite aussi le passage à l'échelle (par exemple pour des calculs lourds à moindre frais vous pouvez vous tourner ves [Google Collab](https://realpython.com/python-virtual-environments-a-primer/), mais attention à la politique d'usage des données) et vous trouverez plus facilement de la documentation et des cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considérations pratiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rendre ce document plus agréable à lire, il est présenté sous forme de notebook [Jupyter](https://jupyter.org/), que vous avez probablement déjà rencontré. C'est un format qui a ses avantages (pratique et lisible) et ses inconvénients (difficile à automatiser, mal versionnable, parfois sujet à erreurs). Tout ce qui est fait ici est tout à fait faisable dans un script indépendant et je vous encourage à écrire un tel script plutôt qu'à réutiliser des notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir lancer vous-mêmes et modifier ce notebook, il vous faudra installer quelques dépendances. Je recommande de le faire dans un [environnement virtuel](https://realpython.com/python-virtual-environments-a-primer/) afin de ne pas polluer votre installation principale de Python, mais à défaut, vous pouvez lancer l'appel suivant à `pip` avec le flag `--user`.\n",
    "\n",
    "**NE JAMAIS UTILISER PIP AVEC `sudo`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour installer les dépendances, ouvrez un terminal et entrez\n",
    "\n",
    "```console\n",
    "pip install jupyter scikit-learn\n",
    "```\n",
    "\n",
    "Puis démarrez jupyter\n",
    "\n",
    "```console\n",
    "jupyter notebook demos/classier_with_sklearn.ipynb\n",
    "```\n",
    "\n",
    "Celà devrait vous ouvrir une fenêtre de navigateur sur le notebook, qui est alors prêt à être utilisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va travailler avec le petit corpus habituel. Récupérez le sur [la page du cours](https://loicgrobol.github.io/intro-fouille-textes) ou utilisez la commande suivante pour le télécharge dans un dossier `local/corpus` dans le même dossier que le notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p local && wget -qO- \"https://github.com/LoicGrobol/intro-fouille-textes/releases/download/stable/sample-data.tar.gz\" | tar xz -C local --strip-components=2 data/expl3/corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listons les fichiers du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local/corpus:\n",
      "\u001b[0m\u001b[01;34mculture\u001b[0m  \u001b[01;34msociété\u001b[0m\n",
      "\n",
      "local/corpus/culture:\n",
      "\u001b[00;32mdoc0.txt\u001b[0m   \u001b[00;32mdoc12.txt\u001b[0m  \u001b[00;32mdoc15.txt\u001b[0m  \u001b[00;32mdoc3.txt\u001b[0m  \u001b[00;32mdoc6.txt\u001b[0m  \u001b[00;32mdoc9.txt\u001b[0m\n",
      "\u001b[00;32mdoc10.txt\u001b[0m  \u001b[00;32mdoc13.txt\u001b[0m  \u001b[00;32mdoc1.txt\u001b[0m   \u001b[00;32mdoc4.txt\u001b[0m  \u001b[00;32mdoc7.txt\u001b[0m\n",
      "\u001b[00;32mdoc11.txt\u001b[0m  \u001b[00;32mdoc14.txt\u001b[0m  \u001b[00;32mdoc2.txt\u001b[0m   \u001b[00;32mdoc5.txt\u001b[0m  \u001b[00;32mdoc8.txt\u001b[0m\n",
      "\n",
      "local/corpus/société:\n",
      "\u001b[00;32mdoc0.txt\u001b[0m   \u001b[00;32mdoc12.txt\u001b[0m  \u001b[00;32mdoc15.txt\u001b[0m  \u001b[00;32mdoc3.txt\u001b[0m  \u001b[00;32mdoc6.txt\u001b[0m  \u001b[00;32mdoc9.txt\u001b[0m\n",
      "\u001b[00;32mdoc10.txt\u001b[0m  \u001b[00;32mdoc13.txt\u001b[0m  \u001b[00;32mdoc1.txt\u001b[0m   \u001b[00;32mdoc4.txt\u001b[0m  \u001b[00;32mdoc7.txt\u001b[0m\n",
      "\u001b[00;32mdoc11.txt\u001b[0m  \u001b[00;32mdoc14.txt\u001b[0m  \u001b[00;32mdoc2.txt\u001b[0m   \u001b[00;32mdoc5.txt\u001b[0m  \u001b[00;32mdoc8.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ls -Rh local/corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer le package [`scikit-learn`](https://scikit-learn.org), un très bon outil pour l'apprentissage artificiel en Python, sur lequel va s'appuyer la suite de cette démo.\n",
    "\n",
    "Notez que le nom à utiliser pour l'import est bien `sklearn` pas `scikit-learn`. On importe également les sous-modules qui nous seront utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On import maintenant les données : `scikit-learn` a pour ça une commande toute prête, [`sklearn.datasets.load_files`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html), qui prend en entrée exactement le même format que le script de vectorisation — un dossier par classe, un fichier texte par document. `shuffle` sert ici à mélanger les documents pour ne pas avoir tous les documents d'une même classe les uns à la suite des autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sklearn.datasets.load_files(\"local/corpus\", encoding=\"utf-8\", shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet renvoyé est de type [`sklearn.utils.Bunch`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html) et est prévu pour permettre facilement d'accéder aux documents via son attribut `data`. Ici les 100 premiers caractères du premier document :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"La française recouvre d'abord l'ensemble des pratiques qui se trouvent sur le territoire français. M\""
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classes sont données par `target` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 0, 0, 1])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les indices correspondants aux noms dans `target_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['culture', 'société']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étape suivant est de vectoriser notre corpus : transformer chaque document en une liste de nombres comme on l'a vu dans le cours. `scikit-learn` fournit l'utilitaire [`sklearn.feature_extraction.text.CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), très complet qui permet d'appliquer une segmentation basique et de transformer des chaînes de caractères en sacs de mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer le sous-module dans lequel il se trouve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis appliquons-le à nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le corpus a 32 lignes et 11415 colonnes\n",
      "Et voici un échantillon :\n"
     ]
    },
    {
     "data": {
      "text/plain": "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "counts = vectorizer.fit_transform(dataset.data)\n",
    "print(f\"Le corpus a {counts.shape[0]} lignes et {counts.shape[1]} colonnes\")\n",
    "print(\"Et voici un échantillon :\")\n",
    "counts[:6, :15].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat est une matrice, stockée sous forme creuse pour économiser de la mémoire (on ne stocke que les valeurs non-nulles), d'où l'appel à `todense` pour l'afficher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` a de nombreuses options de prétraitement, et je vous encourage à aller lire [sa documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) et à expérimenter avec les paramètres. Par exemple pour filtrer les hapax, on peut fixer `min_df` *minimum document frequency* à 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=2)\n",
    "counts = vectorizer.fit_transform(dataset.data)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons enfin qu'on utilise ici la fonction `fit_transformer` qui combine deux opérations\n",
    "\n",
    "- `fit`, qui construit le vocabulaire\n",
    "- `transform`, qui vectorise les données en utilisant ce vocabulaire\n",
    "\n",
    "On peut aussi les utiliser à part. Par exemple ici on ne construit le vocabulaire que sur les 10 premiers textes, mais on vectorise tout :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le corpus sans les hapax et avec vocabulaire réduit a 32 lignes et 1370 colonnes\n"
     ]
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=2)\n",
    "vectorizer.fit(dataset.data[:10])\n",
    "counts_only10 = vectorizer.transform(dataset.data)\n",
    "print(f\"Le corpus sans les hapax et avec vocabulaire réduit a {counts_only10.shape[0]} lignes et {counts_only10.shape[1]} colonnes\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle que soit la façon dont on le fait, un fois que le vectoriser a été `fit`é, on peut le réappliquer à d'autres données : en particulier pour votre corpus de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour construire un corpus de test on va se contenter d'utiliser un simple split aléatoire, mais il y a aussi moyen de faire de la validation croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "matrix([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 1, 0],\n        ...,\n        [1, 1, 0, ..., 0, 1, 1],\n        [4, 3, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set, y_train, y_test = sklearn.model_selection.train_test_split(dataset.data, dataset.target, test_size=0.33)\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=2)\n",
    "train_set_counts = vectorizer.fit_transform(train_set)\n",
    "test_set_counts = vectorizer.transform(test_set)\n",
    "test_set_counts.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner des classifieurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('introfdt': virtualenvwrapper)",
   "name": "python392jvsc74a57bd0597e74b4e4e9f0b62277f790bb6634f1fa5b16dbd8a83078c1a510c5329e057a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "597e74b4e4e9f0b62277f790bb6634f1fa5b16dbd8a83078c1a510c5329e057a"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}