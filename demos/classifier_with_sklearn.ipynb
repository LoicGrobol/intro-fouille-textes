{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprendre des classifieurs avec `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce cours, aucune compétence en programmation n'est nécessaire, et tout peut se faire avec weka et le script de vectorisation fourni. Cependant, en dehors de ce cours, il est souvent utile de savoir utiliser un langage de script comme Python pour faire de la classification de documents par apprentissage artificiel.\n",
    "\n",
    "Celà permet par exemple de modifier et d'étendre facilement les prétraitements, et de réaliser ces derniers et l'apprentissage et l'évaluation de classifieurs dans le même environnement. Cela facilite aussi le passage à l'échelle (par exemple pour des calculs lourds à moindre frais vous pouvez vous tourner ves [Google Collab](https://realpython.com/python-virtual-environments-a-primer/), mais attention à la politique d'usage des données) et vous trouverez plus facilement de la documentation et des cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considérations pratiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rendre ce document plus agréable à lire, il est présenté sous forme de notebook [Jupyter](https://jupyter.org/), que vous avez probablement déjà rencontré. C'est un format qui a ses avantages (pratique et lisible) et ses inconvénients (difficile à automatiser, mal versionnable, parfois sujet à erreurs). Tout ce qui est fait ici est tout à fait faisable dans un script indépendant et je vous encourage à écrire un tel script plutôt qu'à réutiliser des notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir lancer vous-mêmes et modifier ce notebook, il vous faudra installer quelques dépendances. Je recommande de le faire dans un [environnement virtuel](https://realpython.com/python-virtual-environments-a-primer/) afin de ne pas polluer votre installation principale de Python, mais à défaut, vous pouvez lancer l'appel suivant à `pip` avec le flag `--user`.\n",
    "\n",
    "**NE JAMAIS UTILISER PIP AVEC `sudo`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour installer les dépendances, ouvrez un terminal et entrez\n",
    "\n",
    "```console\n",
    "pip install jupyter scikit-learn\n",
    "```\n",
    "\n",
    "Puis démarrez jupyter\n",
    "\n",
    "```console\n",
    "jupyter notebook demos/classier_with_sklearn.ipynb\n",
    "```\n",
    "\n",
    "Celà devrait vous ouvrir une fenêtre de navigateur sur le notebook, qui est alors prêt à être utilisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va travailler avec le petit corpus habituel. Récupérez le sur [la page du cours](https://loicgrobol.github.io/intro-fouille-textes) ou utilisez la commande suivante pour le télécharge dans un dossier `local/corpus` dans le même dossier que le notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p local && wget -qO- \"https://github.com/LoicGrobol/intro-fouille-textes/releases/download/stable/sample-data.tar.gz\" | tar xz -C local --strip-components=2 data/expl3/corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listons les fichiers du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local/corpus:\n",
      "\u001b[0m\u001b[01;34mculture\u001b[0m  \u001b[01;34msociété\u001b[0m\n",
      "\n",
      "local/corpus/culture:\n",
      "\u001b[00;32mdoc0.txt\u001b[0m   \u001b[00;32mdoc12.txt\u001b[0m  \u001b[00;32mdoc15.txt\u001b[0m  \u001b[00;32mdoc3.txt\u001b[0m  \u001b[00;32mdoc6.txt\u001b[0m  \u001b[00;32mdoc9.txt\u001b[0m\n",
      "\u001b[00;32mdoc10.txt\u001b[0m  \u001b[00;32mdoc13.txt\u001b[0m  \u001b[00;32mdoc1.txt\u001b[0m   \u001b[00;32mdoc4.txt\u001b[0m  \u001b[00;32mdoc7.txt\u001b[0m\n",
      "\u001b[00;32mdoc11.txt\u001b[0m  \u001b[00;32mdoc14.txt\u001b[0m  \u001b[00;32mdoc2.txt\u001b[0m   \u001b[00;32mdoc5.txt\u001b[0m  \u001b[00;32mdoc8.txt\u001b[0m\n",
      "\n",
      "local/corpus/société:\n",
      "\u001b[00;32mdoc0.txt\u001b[0m   \u001b[00;32mdoc12.txt\u001b[0m  \u001b[00;32mdoc15.txt\u001b[0m  \u001b[00;32mdoc3.txt\u001b[0m  \u001b[00;32mdoc6.txt\u001b[0m  \u001b[00;32mdoc9.txt\u001b[0m\n",
      "\u001b[00;32mdoc10.txt\u001b[0m  \u001b[00;32mdoc13.txt\u001b[0m  \u001b[00;32mdoc1.txt\u001b[0m   \u001b[00;32mdoc4.txt\u001b[0m  \u001b[00;32mdoc7.txt\u001b[0m\n",
      "\u001b[00;32mdoc11.txt\u001b[0m  \u001b[00;32mdoc14.txt\u001b[0m  \u001b[00;32mdoc2.txt\u001b[0m   \u001b[00;32mdoc5.txt\u001b[0m  \u001b[00;32mdoc8.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ls -Rh local/corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer le package [`scikit-learn`](https://scikit-learn.org), un très bon outil pour l'apprentissage artificiel en Python, sur lequel va s'appuyer la suite de cette démo.\n",
    "\n",
    "Notez que le nom à utiliser pour l'import est bien `sklearn` pas `scikit-learn`. On importe également les sous-modules qui nous seront utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On import maintenant les données : `scikit-learn` a pour ça une commande toute prête, [`sklearn.datasets.load_files`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html), qui prend en entrée exactement le même format que le script de vectorisation — un dossier par classe, un fichier texte par document. `shuffle` sert ici à mélanger les documents pour ne pas avoir tous les documents d'une même classe les uns à la suite des autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sklearn.datasets.load_files(\"local/corpus\", encoding=\"utf-8\", shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet renvoyé est de type [`sklearn.utils.Bunch`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html) et est prévu pour permettre facilement d'accéder aux documents via son attribut `data`. Ici les 100 premiers caractères du premier document :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"La française recouvre d'abord l'ensemble des pratiques qui se trouvent sur le territoire français. M\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classes sont données par `target` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les indices correspondants aux noms dans `target_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['culture', 'société']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étape suivant est de vectoriser notre corpus : transformer chaque document en une liste de nombres comme on l'a vu dans le cours. `scikit-learn` fournit l'utilitaire [`sklearn.feature_extraction.text.CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), très complet qui permet d'appliquer une segmentation basique et de transformer des chaînes de caractères en sacs de mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer le sous-module dans lequel il se trouve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis appliquons-le à nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le corpus a 32 lignes et 11415 colonnes\n",
      "Et voici un échantillon :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "counts = vectorizer.fit_transform(dataset.data)\n",
    "print(f\"Le corpus a {counts.shape[0]} lignes et {counts.shape[1]} colonnes\")\n",
    "print(\"Et voici un échantillon :\")\n",
    "counts[:6, :15].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat est une matrice, stockée sous forme creuse pour économiser de la mémoire (on ne stocke que les valeurs non-nulles), d'où l'appel à `todense` pour l'afficher, en interne elle est représentée par une liste de triplets $(\\text{ligne}, \\text{colonne}, \\text{valeur})$ où on ne représente que les valeurs non-nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 5)\t2\n",
      "  (1, 6)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 10)\t2\n",
      "  (4, 8)\t1\n",
      "  (5, 1)\t1\n",
      "  (5, 4)\t2\n"
     ]
    }
   ],
   "source": [
    "print(counts[:6, :15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` a de nombreuses options de prétraitement, et je vous encourage à aller lire [sa documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) et à expérimenter avec les paramètres. Par exemple pour filtrer les hapax, on peut fixer `min_df` *minimum document frequency* à 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le corpus a 32 lignes et 4404 colonnes\n",
      "Et voici un échantillon :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "        [0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 2, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 4, 0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=2)\n",
    "counts = vectorizer.fit_transform(dataset.data)\n",
    "print(f\"Le corpus a {counts.shape[0]} lignes et {counts.shape[1]} colonnes\")\n",
    "print(\"Et voici un échantillon :\")\n",
    "counts[:6, :15].todense()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons enfin qu'on utilise ici la fonction `fit_transformer` qui combine deux opérations\n",
    "\n",
    "- `fit`, qui construit le vocabulaire\n",
    "- `transform`, qui vectorise les données en utilisant ce vocabulaire\n",
    "\n",
    "On peut aussi les utiliser à part. Par exemple ici on ne construit le vocabulaire que sur les 10 premiers textes, mais on vectorise tout :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le corpus sans les hapax et avec vocabulaire réduit a 32 lignes et 1370 colonnes\n"
     ]
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=2)\n",
    "vectorizer.fit(dataset.data[:10])\n",
    "counts_only10 = vectorizer.transform(dataset.data)\n",
    "print(f\"Le corpus sans les hapax et avec vocabulaire réduit a {counts_only10.shape[0]} lignes et {counts_only10.shape[1]} colonnes\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle que soit la façon dont on le fait, un fois que le vectoriser a été `fit`é, on peut le réappliquer à d'autres données : en particulier pour votre corpus de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour construire un corpus de test on va se contenter d'utiliser un simple split aléatoire, mais il y a aussi moyen de faire de la validation croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [3, 0, 1, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0],\n",
       "        ...,\n",
       "        [0, 4, 0, ..., 2, 0, 2],\n",
       "        [3, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set, train_target, test_target = sklearn.model_selection.train_test_split(dataset.data, dataset.target, test_size=0.33)\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=2)\n",
    "train_set_counts = vectorizer.fit_transform(train_set)\n",
    "test_set_counts = vectorizer.transform(test_set)\n",
    "test_set_counts.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner des classifieurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rentre maintenant dans le vif du sujet : comment utiliser les données pour évaluer et tester un classifieur.\n",
    "\n",
    "`scikit-learn` a en natif la plupart des classifieurs qu'on peut avoir envie d'utiliser. Commençons avec Naïve Bayes. La version qu'on a vu en cours est disponible dans [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer et entraîner un classifier ça marche exactement comme pour un vectoriser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = sklearn.naive_bayes.MultinomialNB()\n",
    "classifier.fit(train_set_counts, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut alors utiliser le classifieur pour classer de nouvelles données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(test_set_counts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut retrouver le nom de sa classe prédite et la comparer à sa classe gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe réelle: société, classe prédite culture\n"
     ]
    }
   ],
   "source": [
    "predicted_class = dataset.target_names[classifier.predict(test_set_counts[0]).item()]\n",
    "gold_class = dataset.target_names[test_target[0].item()]\n",
    "print(f\"Classe réelle: {gold_class}, classe prédite {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on fait ça sur tout le jeu de test, on récupère l'exactitude du classifieur (en important d'abord `numpy` pour manipuler les résultats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicted = classifier.predict(test_set_counts)\n",
    "np.mean(test_predicted == test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour une évaluation plus détaillée, `scikit-learn` fournit directement les métriques de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.33      1.00      0.50         3\n",
      "     société       1.00      0.25      0.40         8\n",
      "\n",
      "    accuracy                           0.45        11\n",
      "   macro avg       0.67      0.62      0.45        11\n",
      "weighted avg       0.82      0.45      0.43        11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(test_target, test_predicted, target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquer des transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également appliquer des transformations aux données, par exemple tf⋅idf, qu'on a vu en cours est disponible comme [`sklearn.feature_extraction.text.TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html), qui est à appliquer après la vectorisation et s'utilise de façon similaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "train_set_tfidf = tfidf_transformer.fit_transform(train_set_counts)\n",
    "test_set_tfidf = tfidf_transformer.transform(test_set_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère alors des représentations qu'on peux apprendre à l'aide d'un classifieur, comme précédemment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.33      1.00      0.50         3\n",
      "     société       1.00      0.25      0.40         8\n",
      "\n",
      "    accuracy                           0.45        11\n",
      "   macro avg       0.67      0.62      0.45        11\n",
      "weighted avg       0.82      0.45      0.43        11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_tfidf = sklearn.naive_bayes.MultinomialNB()\n",
    "classifier_tfidf.fit(train_set_tfidf, train_target)\n",
    "test_tfidf_predicted = classifier.predict(test_set_tfidf)\n",
    "print(sklearn.metrics.classification_report(test_target, test_tfidf_predicted, target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autres classifieurs du cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors, sklearn.tree, sklearn.svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour nous faciliter la vie : une fonction qui apprend et teste un classifieur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(classifier, train_set, train_target, test_set, test_target, target_names):\n",
    "    classifier.fit(train_set, train_target)\n",
    "    test_predicted = classifier.predict(test_set)\n",
    "    print(sklearn.metrics.classification_report(test_target, test_predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'on va appliquer à tous les classifieurs vus en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.20      0.67      0.31         3\n",
      "     société       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.18        11\n",
      "   macro avg       0.10      0.33      0.15        11\n",
      "weighted avg       0.05      0.18      0.08        11\n",
      "\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.43      1.00      0.60         3\n",
      "     société       1.00      0.50      0.67         8\n",
      "\n",
      "    accuracy                           0.64        11\n",
      "   macro avg       0.71      0.75      0.63        11\n",
      "weighted avg       0.84      0.64      0.65        11\n",
      "\n",
      "MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.33      1.00      0.50         3\n",
      "     société       1.00      0.25      0.40         8\n",
      "\n",
      "    accuracy                           0.45        11\n",
      "   macro avg       0.67      0.62      0.45        11\n",
      "weighted avg       0.82      0.45      0.43        11\n",
      "\n",
      "LinearSVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.22      0.67      0.33         3\n",
      "     société       0.50      0.12      0.20         8\n",
      "\n",
      "    accuracy                           0.27        11\n",
      "   macro avg       0.36      0.40      0.27        11\n",
      "weighted avg       0.42      0.27      0.24        11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in (\n",
    "    sklearn.neighbors.KNeighborsClassifier(n_neighbors=3),\n",
    "    sklearn.tree.DecisionTreeClassifier(),\n",
    "    sklearn.naive_bayes.MultinomialNB(),\n",
    "    sklearn.svm.LinearSVC(),\n",
    "):\n",
    "    print(type(classifier).__name__)\n",
    "    train_eval(\n",
    "        classifier,\n",
    "        train_set_counts,\n",
    "        train_target,\n",
    "        test_set_counts,\n",
    "        test_target,\n",
    "        dataset.target_names,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "597e74b4e4e9f0b62277f790bb6634f1fa5b16dbd8a83078c1a510c5329e057a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
