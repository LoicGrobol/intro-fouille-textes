% LTeX: language=fr
% Copyright © 2019, Loïc Grobol <loic.grobol@gmail.com>
% This document is available under the terms of the Creative Commons Attribution 4.0 International License (CC BY 4.0) (https://creativecommons.org/licenses/by/4.0/)

\documentclass[../allslides.tex]{subfiles}

\begin{document}

\renewcommand\docdate{2021-02-18}  % chktex 8

\lecture{Arbres de décision}{Cours 8}


\begin{frame}[fragile=singleslide]{Exercice}
	Étant donnée la représentation tabulaire suivante d'un corpus, attribuez les classes manquantes par l'heuristique des $k$-plus proches voisins
	\begin{table}
		\begin{tabu} to .9\linewidth {*{4}{c}}
			\texttt{t}  & \texttt{wug} & \texttt{neurone} & \texttt{classe}\\
			\hline
			\texttt{1}  & \texttt{3}      & \texttt{2}	& linguistique\\
			\texttt{2}  & \texttt{1}      & \texttt{0}	& linguistique\\
			\texttt{3}  & \texttt{0}      & \texttt{2}	& linguistique\\
			\texttt{4}  & \texttt{2}      & \texttt{1}	& linguistique\\
			\texttt{5}  & \texttt{4}      & \texttt{4}	& informatique\\
			\texttt{6}  & \texttt{1}      & \texttt{3}	& informatique\\
			\texttt{7}  & \texttt{1}      & \texttt{2}	& informatique\\
			\texttt{8}  & \texttt{2}      & \texttt{2}	& ?\\
			\texttt{9}  & \texttt{1}      & \texttt{4}	& ?\\
			\texttt{10}  & \texttt{3}      & \texttt{1}	& ?\\
		\end{tabu}
	\end{table}
\end{frame}

\begin{frame}[fragile]{Exercice}
    \begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}
            \draw [help lines, xstep=1cm, ystep=1cm] (0, 0) grid (5.25, 4.25);

            \draw[->] (-0.5,0) -- (5.25, 0);
            \foreach \x in {1,...,5}
                \draw[shift={(\x, 0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};

            \draw[->] (0, -0.5) -- (0, 4.25);
            \foreach \y in {1,...,4}
                \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};

            \draw (0, 0) node[below left] {\footnotesize $0$};

            \foreach \x/\y/\c in {3/2/highlight4,1/0/highlight4,0/2/highlight4,2/1/highlight4,4/4/highlighta,1/3/highlighta,1/2/highlighta}{
                \path[fill=\c] (\x, \y) circle[radius=3pt];
            }

            \foreach \ex/\ey [count=\i from 1] in {2/2,1/4,3/1} {
                \path[fill=highlight6, visible on=2] (\ex, \ey) circle[radius=3pt];
            }
        \end{tikzpicture}
        \caption{Représentation du corpus dans le plan}
    \end{figure}
    \only<2>{}
\end{frame}


\section{Arbres de décision}
\begin{frame}{Arbres de décision}
    \textbf{Dans Weka} trees (> J48)

    \textbf{Espace de recherche} L'ensemble des arbres de recherche pour les attributs choisis

    % TODO: cite
    \textbf{Techniques de recherche} Plusieurs algorithmes, le plus connu étant C4.5, appelé J48 dans Weka
\end{frame}

\begin{frame}{Arbre de décision}
    Modèle de prise de décision déterministe
    \begin{figure}
        \tikzset{external/export=true}
        \begin{forest}
            for tree={draw, l sep=2em, s sep=3em}
            [{fouille de textes ?},ellipse
				[{dormir},edge label={node[midway,left] {oui}}]
				[{écouter},edge label={node[midway,right] {non}}]
            ]
        \end{forest}
        \caption{Prise de décision standard en M1 PluriTAL}
    \end{figure}
\end{frame}

\begin{frame}{Arbre de décision}
    Modèle de prise de décision déterministe par succession de choix
    \begin{figure}
        \tikzset{external/export=true}
        \begin{forest}
            for tree={draw, l sep=2em, s sep=3em}
            [{fouille de textes ?},ellipse
				[semaine,ellipse,edge label={node[midway,left] {oui}}
					[{écouter},edge label={node[midway,left] {$⩽3$}}]
					[{dormir},edge label={node[midway,right] {$>3$}}]
				]
				[{écouter},edge label={node[midway,right] {non}}]
            ]
	    \end{forest}
	    \caption{Prise de décision standard en M1 PluriTAL}
	\end{figure}
\end{frame}

\begin{frame}{Arbre de décision}
    Modèle de prise de décision déterministe par succession de choix (pas nécessairement binaires)
    \begin{figure}
        \tikzset{external/export=true}
        \begin{forest}
            for tree={draw, l sep=2em, s sep=3em}
            [{fouille de textes ?},ellipse
				[semaine,ellipse,edge label={node[midway,left] {oui}}
					[{écouter},edge label={node[midway,left] {$⩽3$}}]
					[{dormir},edge label={node[midway] {$∈[4,9]$}}]
					[{écouter},edge label={node[midway,right] {$>10$}}]
				]
				[{écouter},edge label={node[midway,right] {non}}]
            ]
	    \end{forest}
	    \caption{Prise de décision standard en M1 PluriTAL}
	\end{figure}
\end{frame}

\begin{frame}[fragile=singleslide]{Exercice}
	Construire un arbre de décision pour les données suivantes
	\vspace{-\bigskipamount}
	\begin{table}
		\small
		\begin{tabu} to .9\linewidth {*{6}{c}}
			No  & outlook  & humidity & windy & play\\
			\hline
			1 & sunny & high & false & no\\
			2 & sunny & high & true & no\\
			3 & overcast & high & false & yes\\
			4 & rainy & high & false & yes\\
			5 & rainy & high & false & yes\\
			6 & rainy & low & true & no\\
			7 & overcast & low & true & yes\\
			8 & sunny & high & false & no\\
			9 & sunny & low & false & yes\\
			10 & rainy & high & false & yes\\
			11 & sunny & low & true & yes\\
			12 & overcast & high & true & yes\\
			13 & overcast & low & false & yes\\
			14 & rainy & high & true & no\\
		\end{tabu}
	\end{table}
\end{frame}

% TODO: iterative construction with returning to the table to explain
\begin{frame}[fragile]
	\begin{figure}
        \tikzset{external/export=true}
        \begin{forest}
            for tree={draw, l sep=2em, s sep=3em}
            [outlook,ellipse
                [humidity,ellipse,edge label={node[midway,left] {sunny}}
                    [yes,edge label={node[midway,left] {low}}]
                    [no,edge label={node[midway,right] {high}}]
                ]
                [yes,edge label={node[midway] {overcast}}]
                [windy,ellipse,edge label={node[midway,right] {rainy}}
                    [yes,edge label={node[midway,left] {true}}]
                    [no,edge label={node[midway,right] {false}}]
                ]
            ]
        \end{forest}
        \caption{Arbre de décision pour \emph{weather}}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Arbre de décision}
    En Python
    \begin{figure}
        \begin{minted}{python}
            def classify(outlook, humidity, windy):
                if outlook == "sunny":
                    if humidity == "low":
                        return "yes"
                    else:
                        return "no"
                elif outlook == "overcast":
                    return "yes"
                elif outlook == "rainy":
                    if windy:
                        return "yes"
                    else:
                        return "no"
        \end{minted}
        \caption{Code correspondant à l'arbre précédent}
    \end{figure}
\end{frame}

\begin{frame}{Technique d'apprentissage}
    C'est assez facile de construire un arbre parfait pour l'ensemble d'entraînement:

    \begin{itemize}
        \item<+-> Il suffit d'énumérer tous les attributs jusqu'à avoir généré toutes les combinaisons existantes
        \item<+->[→] Surapprentissage ! % TODO: noter que le problème c'est bien d'avoir un arbre trop profond, suggérer un hard cur sur la profondeur
    \end{itemize}
	\itpause
    Comment faire pour avoir un arbre bon, mais pas trop profond ?
    \pause
    \begin{itemize}
        \item Faire en sorte de trier vite et bien
        \item[→] En choisissant les attributs les plus discriminants
    \end{itemize}
    L'idée est de construire l'arbre progressivement, prenant à chaque étape le test le plus \alert{discriminant}, reste à savoir comment on le détermine.
\end{frame}

% FIXME: Review this whole part : start by growing the decision tree for the weather dataset by hand
% thus motivating both the Gini indice definition and the general procedure and only formalize
% things after. Plan on using real attribute names and values instead of abstract ones, use concrete
% examples EVERYWHERE and add a lot of figures (possibly animated) e.g. the change S→S_{a,v}
% graphically. Only introduce entropy after all that
\begin{frame}{Indice de diversité de Gini}
    \begin{block}{Définition}
        On appelle \emph{indice de diversité de Gini} d'une partition $S=⨆_{1⩽i⩽n}c_i$
        \begin{equation}
            Gini(S) = ∑_{1⩽i⩽n}p_i(1-p_i)
        \end{equation}
        avec $p_i=\frac{\#c_i}{∑_{j=1}^n\#c_j}$
    \end{block}
    Autrement dit, l'indice de diversité de Gini est la probabilité qu'un exemple choisi au hasard et classé au hasard soit mal classé.

	Cet indice est d'autant plus élevé que la partition sépare bien les éléments.
\end{frame}

\begin{frame}{Indice de diversité de Gini}
    \begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}
            \begin{axis}[
				xmin=0, xmax=1,
				ymin=0,
				xlabel={$p_1$},
				ylabel={$Gini(S)$},
				width=0.9\textwidth,
				height=0.8\textheight,
				title={Indice de diversité de Gini : $y=x(1-x)+(1-x)x$},
				scaled ticks=false,
			]
                \addplot[highlighta, domain=0:1, samples=1001] {x*(1-x)+(1-x)*x};
            \end{axis}
        \end{tikzpicture}
        \caption{Indice de diversité de Gini pour un problème à deux classes}
    \end{figure}
\end{frame}

\begin{frame}{Indice de diversité de Gini}
    \begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}
            \begin{axis}[xmin=0, xmax=1, ymin=0,
                         width=0.9\textwidth,
                         height=0.8\textheight,
                         xlabel={$p_1$},
                         ylabel={$H(S)$},
                         title={Entropie : $y=-x\log₂(x)-(1-x)\log₂(1-x)$},
                         scaled ticks=false,]
                \addplot[highlighta, domain=0:1, samples=1001] {-x*ln(x)/ln(2)-(1-x)*ln(1-x)/ln(2)};
            \end{axis}
        \end{tikzpicture}
        \caption{Entropie pour un problème à deux classes}
    \end{figure}
\end{frame}

\begin{frame}{Choix des attributs}
    On peut en déduire une valuation de « être un attribut discriminant » : « Générer une partition équilibrée »
    \begin{itemize}
         \item Soit un attribut $a$ à valeurs discrètes
         \item Pour toute valeur $v$ prise par $a$, on note $S_{a=v}$ l'ensemble des éléments de $S$ pour lesquels $a$ vaut $v$.
    \end{itemize}
    On définit lors le gain associé à $a$ par
    \begin{equation}
        g(S, a) = Gini(S) - ∑_i\frac{\#S_{a=v}}{\#S}Gini(S_{a=v})
    \end{equation}
    \begin{itemize}
        \item Un attribut est d'autant plus discriminant que son gain est élevé.
        \item On peut procéder de même avec $H$ (ou un autre indice de diversité)
    \end{itemize}
\end{frame}

\begin{frame}{Entropie}
    \begin{block}{Définition}
        On appelle \emph{entropie} d'une partition $S=⨆_{1⩽i⩽n}c_i$
        \begin{equation}
            H(S) = -∑_{1⩽i⩽n}p_i\log₂(p_i)
        \end{equation}
        avec $p_i=\frac{\#c_i}{∑_{j=1}^n\#c_j}$
    \end{block}
    Intuitivement, si on choisit au hasard et de façon uniforme un exemple $x$ dans $S$
    \begin{itemize}
        \item $p_i$ est la probabilité de l'évènement « La classe de $x$ est $c_i$ »
        \item $-\log₂p_i$ mesure la surprise de l'évènement « La classe de $x$ est $c_i$ »
    \end{itemize}
    Finalement, $H$ est donc la surprise moyenne de $S$.
\end{frame}

\begin{frame}{Attributs à valeurs scalaires}
    Pour des attributs numériques, on se ramène à un choix discret en utilisant des seuils
    \begin{itemize}
        \item Pour un attribut $a$ à valeur numérique $∈[α, β]$ et $r∈[α, β]$, on note $a_r$ l'attribut booléen $a(x)⩽r$
        \item On choisit $s$ tel que $g(S, a_r)$ soit maximal
    \end{itemize}
    On peut ensuite utiliser $a_r$ au lieu de $a$ dans notre choix d'attributs.
\end{frame}

\begin{frame}[fragile]{Représentation graphique des seuils}
    \begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}
            \draw [help lines, xstep=1cm, ystep=1cm] (0, 0) grid (5.25, 4.25);

            \draw[->] (-0.5,0) -- (5.25, 0);
            \foreach \x in {1,...,5}
                \draw[shift={(\x, 0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
            \draw[->] (0, -0.5) -- (0, 4.25);

            \foreach \y in {1,...,4}
                \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};

            \draw (0, 0) node[below left] {\footnotesize $0$};

            \foreach \x/\y/\c in {2/1/highlight4,4/0/highlight4,1/0/highlight4,2/3/highlight4,0/3/highlighta,1/2/highlighta,0/2/highlighta}{
                \path[fill=\c] (\x, \y) circle[radius=3pt];
            }

            \draw[highlight3, line width=2pt, alt={<2,4>{}{invisible}}] (-0.5, 1.5) -- (5.25, 1.5);
            \draw[highlight2, line width=2pt, visible on=3] (1.5, -0.5) -- (1.5, 4.25);
            \draw[highlight2, line width=2pt, visible on=4] (1.5, 1.5) -- (1.5, 4.25);
        \end{tikzpicture}
        \caption{Exemple de classification par seuils}
    \end{figure}
    \only<4>{}
\end{frame}

\begin{frame}{Propriétés}
    La principale qualité des arbres de décision tient dans leur simplicité
    \begin{itemize}
        \item \emph{White box} : on peut comprendre le résultat
        \item Le modèle est de petite taille et est efficace même avec peu de données
        \item La procédure d'apprentissage est assez proche du raisonnement humain conscient
        \item Facilement combinables
    \end{itemize}
    Leur principal défaut est leur instabilité
    \begin{itemize}
        \item[→] des petites variations dans l'ensemble de test peuvent conduire à des changements importants dans l'arbre
    \end{itemize}
    Des extensions plus performantes, mais moins simples existent (\emph{random forest}…)
\end{frame}


%  █████  ██████  ██████  ███████ ███    ██ ██████  ██ ██   ██
% ██   ██ ██   ██ ██   ██ ██      ████   ██ ██   ██ ██  ██ ██
% ███████ ██████  ██████  █████   ██ ██  ██ ██   ██ ██   ███
% ██   ██ ██      ██      ██      ██  ██ ██ ██   ██ ██  ██ ██
% ██   ██ ██      ██      ███████ ██   ████ ██████  ██ ██   ██

\ifSubfilesClassLoaded{
	\appendix
	\pgfkeys{/metropolis/inner/sectionpage=simple}  % Avoid random errors with section page progressbar
	\section{Annexes}
	\pdfbookmark[2]{Remerciements}{acknowledgements}
	\begin{frame}{Remerciements}
		Ce cours a été construit à partir du polycopié de cours \citetitle{tellier2017IntroductionFouilleTextes} \parencite{tellier2017IntroductionFouilleTextes} et des précieux conseils d'Isabelle Tellier que je ne saurais trop remercier pour sa confiance et son dévouement.
	\end{frame}

	\pdfbookmark[2]{Références}{references}
	\begin{frame}[allowframebreaks]{References}
		\printbibliography[heading=none]
	\end{frame}

	\pdfbookmark[2]{Licence}{licence}
	\begin{frame}{Licence}
		\begin{center}
			{\huge \ccby}
			\vfill
			This document is available under the terms of the Creative Commons Attribution 4.0 International License (CC BY 4.0) (\shorturl{creativecommons.org/licenses/by/4.0})

			Exceptions to the above statement are listed at {\small\shorturl{loicgrobol.github.io/intro-fouille-textes\#licences}}
			\vfill
			© 2019, Loïc Grobol <\shorturl[mailto][:]{loic.grobol@gmail.com}>

			\shorturl[http]{lattice.cnrs.fr/Grobol-Loic}
		\end{center}
	\end{frame}
}{}

\end{document}
