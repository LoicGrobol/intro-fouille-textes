% Copyright © 2018, Loïc Grobol <loic.grobol@gmail.com>
% This document is available under the terms of the Creative Commons Attribution 4.0 International License (CC BY 4.0) (https://creativecommons.org/licenses/by/4.0/)

\RequirePackage{xparse}
\RequirePackage{shellesc}
% Settings
\NewDocumentCommand\myname{}{Loïc Grobol}
\NewDocumentCommand\mylab{}{Lattice / ALMAnaCH}
\NewDocumentCommand\pdftitle{}{Introduction à la fouille de textes}
\NewDocumentCommand\mymail{}{loic.grobol@gmail.com}
\NewDocumentCommand\titlepagetitle{}{\pdftitle}
\NewDocumentCommand\titlepagesubtitle{}{Cours 12 : Modèles d'étiquetage}
\NewDocumentCommand\docdate{}{2019-04-30}
\NewDocumentCommand\conference{}{M1 Plurital}

\documentclass[hyperref={unicode}, xcolor={svgnames}, french]{beamer}
% Alternative navy blue for ⩽4 palettes
\definecolor{highlighta}{RGB}{68, 118, 170}  % Navy blue

% Colour palette from [Paul Tol's technical note](https://personal.sron.nl/~pault/data/colourschemes.pdf) v3.1
% Bright scheme
\definecolor{sronbrighttblue}{RGB}{68, 119, 170}
\definecolor{sronbrightcyan}{RGB}{102, 204, 238}
\definecolor{sronbrightgreen}{RGB}{34, 136, 51}
\definecolor{sronbrightyellow}{RGB}{204, 187, 68}
\definecolor{sronbrightred}{RGB}{238, 102, 119}
\definecolor{sronbrightpurple}{RGB}{170, 51, 119}
\definecolor{sronbrightgrey}{RGB}{187, 187, 187}

\colorlet{highlight0}{sronbrighttblue}
\colorlet{highlight1}{sronbrightred}
\colorlet{highlight2}{sronbrightgreen}
\colorlet{highlight3}{sronbrightyellow}
\colorlet{highlight4}{sronbrightcyan}
\colorlet{highlight5}{sronbrightpurple}
\colorlet{highlight6}{sronbrightgrey}
% Legacy highlights
\definecolor{highlight7}{RGB}{136, 34, 85}    % Purple
\definecolor{highlight8}{RGB}{170, 68, 153}   % Violet


\usetheme[
	sectionpage=progressbar,
    subsectionpage=progressbar,
	progressbar=frametitle,
]{metropolis}
	\colorlet{accent}{sronbrightgreen}
	\setbeamercolor{frametitle}{bg=DarkSlateGrey}
	\setbeamercolor{alerted text}{fg=accent}
	\makeatletter
		\setlength{\metropolis@progressinheadfoot@linewidth}{2pt}
	\makeatother
	% Avoid ugly whitespace below figures
	% \makeatletter
	% 	\renewenvironment{figure}[1][]{%
	% 	\def\@captype{figure}%
	% 	\par\centering}%
	% 	{\par}
	% \makeatother
	% FIXME: not pretty and footnotes are still too big
	\let\footnoterule\relax  % No footnote rule, push down footnote


% Use non-standard fonts
\usefonttheme{professionalfonts}
	\setsansfont[
		BoldFont={Fira Sans SemiBold},
		ItalicFont={Fira Sans Italic},
		BoldItalicFont={Fira Sans Bold Italic},
	]{Fira Sans Book}
\setmonofont[Scale=0.9]{Fira Mono}

% Fix missing glyphs in Fira by delegating to polyglossia/babel
\usepackage{newunicodechar}
\newunicodechar{ }{~}   % U+202F NARROW NO-BREAK SPACE
\newunicodechar{ }{ }  % U+2009 THIN SPACE

% Notes on left screen
% \usepackage{pgfpages}
% \setbeameroption{show notes on second screen=left}


\usepackage[french, english]{babel}
\usepackage{amsfonts,amssymb}
\usepackage{amsmath,amsthm}
\usepackage{mathtools}	% AMS Maths service pack
	\newtagform{brackets}{[}{]}	% Pour des lignes d'équation numérotées entre crochets
	\mathtoolsset{showonlyrefs, showmanualtags, mathic}	% affiche les tags manuels (\tag et \tag*) et corrige le kerning des maths inline dans un bloc italique voir la doc de mathtools
	\usetagform{brackets}	% Utilise le style de tags défini plus haut
\usepackage{lualatex-math}

\usepackage[math-style=french]{unicode-math}
	\setmathfont[Scale=1.1]{Libertinus Math}
\usepackage{newunicodechar}
	\newunicodechar{√}{\sqrt}
\usepackage{mleftright}

\usepackage{tabu}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{multicol}
\usepackage{ccicons}
\usepackage{bookmark}
\usepackage{caption}
    \captionsetup{skip=1ex}
\usepackage[iso]{datetime}

\usepackage{tikz}
	\NewDocumentCommand{\textnode}{O{}mm}{\tikz[remember picture, baseline=(#2.base), inner sep=0pt]{\node[#1] (#2) {#3};}}
    \NewDocumentCommand{\mathnode}{O{}mm}{\tikz[remember picture, baseline=(#2.base), inner sep = 0pt]{\node[#1] (#2) {$\displaystyle #3$};}}
	\tikzset{
		alt/.code args={<#1>#2#3}{%
		\alt<#1>{\pgfkeysalso{#2}}{\pgfkeysalso{#3}} % \pgfkeysalso doesn't change the path
		},
        invisible/.style={opacity=0, fill opacity=0},
		visible on/.style={alt={<#1>{}{invisible}}}
	}
    \usepackage{forest}
    \usepackage{tkz-graph}
    \usepackage[beamer, markings]{hf-tikz}
    \usepackage{tikz-3dplot}
    \usepackage{pgfplots}
        % Due to pgfplots meddling with pgfkeys, we have to redefine alt here.
        \pgfplotsset{
    		alt/.code args={<#1>#2#3}{%
    		\alt<#1>{\pgfkeysalso{#2}}{\pgfkeysalso{#3}} % \pgfkeysalso doesn't change the path
    		},
    	}
        \pgfplotsset{compat=1.15}
        \pgfplotsset{colormap={SRON}{rgb255=(61,82,161) rgb255=(255,250,210) rgb255=(174,28,62)}}

    \usetikzlibrary{matrix}
    \usetikzlibrary{shapes, shapes.geometric}
    \usetikzlibrary{decorations.pathreplacing}
	\usetikzlibrary{positioning, calc, intersections}
    \usetikzlibrary{fit}
    \usetikzlibrary{backgrounds}

    % Do evil things with soft path
    \makeatletter
        \def\@appendnamedsoftpath#1{%
            \pgfsyssoftpath@getcurrentpath\@temppatha
            \expandafter\let\expandafter\@temppathb\csname tikz@intersect@path@name@#1\endcsname
            \expandafter\expandafter\expandafter\def\expandafter\expandafter\expandafter\@temppatha\expandafter\expandafter\expandafter{\expandafter\@temppatha\@temppathb}%
            \pgfsyssoftpath@setcurrentpath\@temppatha
        }
        \def\@appendnamedpathforactions#1{%
            \pgfsyssoftpath@getcurrentpath\@temppatha
            \expandafter\let\expandafter\@temppathb\csname tikz@intersect@path@name@#1\endcsname
            \expandafter\def\expandafter\@temppatha\expandafter{\csname @temppatha\expandafter\endcsname\@temppathb}%\usepackage{tikz-3dplot}

            \let\tikz@actions@path\@temppatha
        }

        \tikzset{
            use path for main/.code={%
                \tikz@addmode{%
                    \expandafter\pgfsyssoftpath@setcurrentpath\csname tikz@intersect@path@name@#1\endcsname
                }%
            },
            append path for main/.code={%
                \tikz@addmode{%
                    \@appendnamedsoftpath{#1}%
                }%
            },
            use path for actions/.code={%
                \expandafter\def\expandafter\tikz@preactions\expandafter{\tikz@preactions\expandafter\let\expandafter\tikz@actions@path\csname tikz@intersect@path@name@#1\endcsname}%
            },
            append path for actions/.code={%
                \expandafter\def\expandafter\tikz@preactions\expandafter{\tikz@preactions
                \@appendnamedpathforactions{#1}}%
            },
            use path/.style={%
                use path for main=#1,
                use path for actions=#1,
            },
            append path/.style={%
                append path for main=#1,
                append path for actions=#1
            }
        }
    \makeatother

    % TikZ externalisation
    \usetikzlibrary{external}
    % Create the `tikzpics/` folder if it does not exist
    \ShellEscape{mkdir -p tikzpics}
    % Only externalise pictures on demand (to avoid messing up with metropolis theme)
    \tikzset{
        external/export=false,
        external/prefix=tikzpics/
    }
    \tikzexternalize

\usepackage{minted}
	\usemintedstyle{lovelace}
	\setminted{autogobble, tabsize=2}
	\setmintedinline{fontsize=auto}

\usepackage{csquotes}

\usepackage[style=authoryear, block=ragged, doi=false, isbn=false]{biblatex}
    \AtEveryBibitem{
        \ifentrytype{online}
        {} {
            \iffieldequalstr{howpublished}{online}
            {
                \clearfield{howpublished}
            } {
                \clearfield{urlyear}\clearfield{urlmonth}\clearfield{urlday}
            }
        }
    }

	\addbibresource{biblio.bib}

% Compact bibliography style
\setbeamertemplate{bibliography item}[text]

\AtEveryBibitem{
    \clearfield{series}
    \clearfield{pages}
    \clearlist{publisher}
    \clearname{editor}
    \clearlist{location}
}
\renewcommand*{\bibfont}{\tiny}

\usepackage{hyperxmp}	% XMP metadata

\usepackage[type={CC}, modifier={by}, version={4.0}]{doclicense}

\usepackage{todonotes}
\let\todox\todo
\renewcommand\todo[1]{\todox[inline]{#1}}

\title{\titlepagetitle}
\subtitle{\titlepagesubtitle}
\author{\textbf{\myname} (\mylab)}
\institute{}
\date{\tiny Version {\yyyymmdddate\today}T\currenttime}

\titlegraphic{\ccby}

% Tikz styles

% Schémas de tâches
\tikzset{
    >=stealth,
    hair lines/.style={line width = 0.05pt, lightgray},
    data/.style={draw, ellipse},
    program/.style={draw, rectangle},
    accent on/.style={alt={<#1>{draw=accent, text=accent, thick}{draw}}},
    true scale/.style={scale=#1, every node/.style={transform shape}},
    highlight/.style={color=highlight#1}
}

% Styles des heatmap pour les moyennes
\pgfplotsset{
    meanheatmap/.style={
        colorbar, colormap name=SRON,
        view={0}{90},
        samples=100,
        domain=0:1,
        min=0, max=1,
        xlabel={$P$},
        ylabel={$R$},
    }
}

% Commands spécifiques
\NewDocumentCommand\shorturl{ O{https} O{://} m }{%
    \href{#1#2#3}{\nolinkurl{#3}}%
}

\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiterX\compset[2]{\lbrace}{\rbrace}{#1\,\delimsize|\,#2}
\DeclarePairedDelimiterX\innprod[2]{\langle}{\rangle}{#1\,\delimsize|\,#2}

\DeclareMathOperator*\argmax{argmax}

% Easy column vectors \vcord{a,b,c} ou \vcord[;]{a;b;c}
% Here be black magic
\ExplSyntaxOn
	\NewDocumentCommand{\vcord}{O{,}m}{\vector_main:nnnn{p}{\\}{#1}{#2}}
	\NewDocumentCommand{\tvcord}{O{,}m}{\vector_main:nnnn{psmall}{\\}{#1}{#2}}
	\seq_new:N\l__vector_arg_seq
	\cs_new_protected:Npn\vector_main:nnnn #1 #2 #3 #4{
		\seq_set_split:Nnn\l__vector_arg_seq{#3}{#4}
		\begin{#1matrix}
			\seq_use:Nnnn\l__vector_arg_seq{#2}{#2}{#2}
		\end{#1matrix}
	}
\ExplSyntaxOff

\DeclareMathOperator{\TF}{TF}
\DeclareMathOperator{\IDF}{IDF}

\ExplSyntaxOn
    \DeclareExpandableDocumentCommand\eval{m}{\fp_eval:n{#1}}
\ExplSyntaxOff


% Fixes for pauses after an unveiled list
\newcommand{\itpause}{%
	\addtocounter{beamerpauses}{-1}%
	\pause
}

\nocite{
	collinsLogLinearModelsMEMMs,
	neubig2019TutorialProgrammingNatural,
	jurafsky2008SpeechLanguageProcessing,
	lafferty2001ConditionalRandomFields
}


% ██████   ██████   ██████ ██    ██ ███    ███ ███████ ███    ██ ████████
% ██   ██ ██    ██ ██      ██    ██ ████  ████ ██      ████   ██    ██
% ██   ██ ██    ██ ██      ██    ██ ██ ████ ██ █████   ██ ██  ██    ██
% ██   ██ ██    ██ ██      ██    ██ ██  ██  ██ ██      ██  ██ ██    ██
% ██████   ██████   ██████  ██████  ██      ██ ███████ ██   ████    ██


\begin{document}
\pdfbookmark[2]{Title}{title}

\begin{frame}[plain]
	\titlepage
\end{frame}

\begin{frame}[fragile=singleslide]{Rappel}
	\begin{quote}
		L'\emph{annotation} ou \emph{étiquetage} est la tâche consistant à attribuer une étiquette à chaque élément d'un ensemble \alert{structuré}
	\end{quote}
	\begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}
            \matrix[matrix of nodes, anchor=base, row sep=1.5em] (m) {
                Le  & petit & chat  & est & content\\
                DET & ADJ   & NC    & V   & ADJ\\
            };
            \foreach \x in {1, ..., 5}
                \draw[->] (m-1-\x) -- (m-2-\x);
        \end{tikzpicture}
        \caption{\emph{Part Of Speech Tagging}}
    \end{figure}
	Ici on parlera essentiellement de l'étiquetage de \alert{séquences}.
\end{frame}

\section{Représenter les données}
\begin{frame}[fragile=singleslide]{Formats}
	On a une tâche de fouille de textes à réaliser

	\begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}
            \matrix[matrix of nodes, anchor=base, row sep=1.5em] (m) {
                Le  & petit & chat  & est & content\\
                DET & ADJ   & NC    & V   & ADJ\\
            };
            \foreach \x in {1, ..., 5}
                \draw[->] (m-1-\x) -- (m-2-\x);
        \end{tikzpicture}
        \caption{\emph{Part Of Speech Tagging}}
    \end{figure}

	La première chose à faire est de représenter les données du problèmes sous forme lisible par la machine.
\end{frame}

\begin{frame}[fragile]{Format CoNLL}
	\begin{columns}
		\column{0.45\textwidth}
			\begin{figure}
				\begin{minted}{text}
					Le	DET
					petit	ADJ
					chat	N
					est	V
					content	ADJ

					Vive	V
					le	DET
					TAL	N
				\end{minted}
			\end{figure}
		\column{0.45\textwidth}
			\begin{itemize}
				\item Un mot par ligne
				\item Colonnes séparées par des tabulateurs
				\item \alert{Séquences séparées par des lignes vides}
			\end{itemize}
	\end{columns}
	Est-ce un format tabulaire au sens habituel ?
\end{frame}

\begin{frame}[fragile]{Indépendance des lignes}
	% This because of the fragility of pygments' parser
	\tikzset{
		squarearound/.style={draw={#1}, line width=2pt, inner sep=3pt},
	}
	\begin{columns}
		\column{0.45\textwidth}
			\begin{figure}
				\begin{minted}[escapeinside=||]{text}
					Le	DET
					petit	|\textnode[squarearound=highlight0]{petitadj}{ADJ}|
					chat	N
					est	V
					content	ADJ
				\end{minted}
			\end{figure}
		\column{0.45\textwidth}
			\begin{figure}
				\begin{minted}[escapeinside=||]{text}
					Le	DET
					petit	|\textnode[squarearound=highlight1]{petitn}{N}|
					mange	V
					la	DET
					crêpe	N
				\end{minted}
			\end{figure}
	\end{columns}
\end{frame}

\begin{frame}{Du format}
	Ce \alert{n'est pas} un format tabulaire au sens précédent de ce cours.

	\begin{itemize}
		\item Les lignes ne sont pas indépendantes
		\item L'ordre des lignes compte
	\end{itemize}

	En revanche
	\begin{itemize}
		\item Les \alert{blocs} sont indépendants
		\item Leur ordre n'est pas significatif
	\end{itemize}

	Moralment, les blocs sont donc ici l'équivalent des lignes des fichiers tabulaires
\end{frame}

\begin{frame}[fragile]{Contexte}
	La non-indépendances des lignes d'un même bloc fait que pour prédire une étiquette, il vaut tenir compte du \alert{contexte}.

	\begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}[
			faded/.style={black!30},
		]
			\matrix (mat) [
				matrix of nodes,
				column sep=2em,
				row sep=1em,
				nodes={align=left, anchor=base west},
			]{
				|[alt=<-2>{faded}{}]| Le & |[faded]| DET\\
				petit & \alt<1>{?}{ADJ}\\
				|[alt=<1>{faded}{}]| chat & |[faded]| N\\
				|[alt=<-2>{faded}{}]| est & |[faded]| V\\
				|[alt=<-2>{faded}{}]| content & |[faded]| ADJ\\
			};
			\draw[->] (mat-2-1) -- (mat-2-2);
			\draw[->, visible on={2-}] (mat-3-1) -- (mat-2-2);
			\foreach \i in {1,4,5}
				\draw[->, visible on=3] (mat-\i-1) -- (mat-2-2);
		\end{tikzpicture}
	\end{figure}
	\only<3>{Mais l'intégralité du contexte n'est pas toujours utile.}
\end{frame}

\begin{frame}[fragile]{Contexte}
	En général on ne tient compte pour chaque étiquette que d'une partie du contexte.

	Par exemple le mot d'avant et le mot d'après
	\begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}[
			faded/.style={black!30},
		]
			\matrix (mat) [
				matrix of nodes,
				row sep=1em,
				column sep=2em,
				column 1/.style={align=right, anchor=base east},
				column 2/.style={align=left, anchor=base west},
			]{
				Le &  DET\\
				petit & ADJ\\
				chat &  N\\
				$⋮$ & $⋮$\\
			};
			\draw[->, visible on=1] (mat-1-1) -- (mat-1-2);
			\draw[->, visible on=1] (mat-2-1) -- (mat-1-2);

			\draw[->, visible on=2] (mat-1-1) -- (mat-2-2);
			\draw[->, visible on=2] (mat-2-1) -- (mat-2-2);
			\draw[->, visible on=2] (mat-3-1) -- (mat-2-2);

			\draw[->, visible on=3] (mat-2-1) -- (mat-3-2);
			\draw[->, visible on=3] (mat-3-1) -- (mat-3-2);
		\end{tikzpicture}
	\end{figure}
	\only<3>{}
\end{frame}

\begin{frame}[fragile=singleslide]{Features contextuelles}
	D'un point de vue apprentissage, « tenir compte du contexte » signifie utiliser les mots du contexte comme \alert{features}.

	Dans un format tabulaire, les dépendances considérées donnent donc
	\begin{table}
		\begin{tabu} to .9\textwidth {cccc}
			[0]	& [-1]	& [1]	& label\\
			\midrule
			Le	& \mintinline{text}{<s>}	& petit	& DET\\
			petit	& Le	& chat	& ADJ\\
			chat	& petit	& est	& N\\
			content	& chat	& \mintinline{text}{</s>}	& ADJ
		\end{tabu}
	\end{table}

	\begin{itemize}
		\item Les lignes sont bien indépendantes
		\item Les features sont des position relative : pour une étiquette donnée, le mot précédent a l'indice $-1$, le mot à étiqueter $0$, le mot suivant $1$ et ainsi de suite.
	\end{itemize}
\end{frame}

\begin{frame}[fragile=singleslide]{Patrons de features}
	On ne se limite pas aux formes pour déterminer une étiquette
	\vspace{-\smallskipamount}
	\begin{table}
		\begin{tabu} to .9\textwidth {ccc}
			Les	& le	& DET\\
			enfants	& enfant	& N\\
			sont	& être	& V\\
			pénibles & pénible	& ADJ
		\end{tabu}
	\end{table}
	\vspace{-\smallskipamount}
	Dans ce cas les features sont précisées en deux dimensions :
	% FIXME: ce tableau est faux
	\begin{table}
		\begin{tabu} to .9\textwidth {ccc}
			\mintinline{text}{[0, 0]}	& \mintinline{text}{[1, 1]}	& label\\
			\midrule
			Les	& enfant 	& DET\\
			enfants	& être 	& ADJ\\
			sont	& pénible 	& N\\
			pénibles	& \mintinline{text}{</s>}	& ADJ
		\end{tabu}
	\end{table}
	\vspace{-1.5\smallskipamount}
	Ici \mintinline{text}{[1, 1]} désigne la valeur de la colonne $1$ pour la ligne suivant l'étiquette.
\end{frame}


\section{Modèles d'étiquetage}
\subsection{Classification indépendante}
\begin{frame}[fragile]{Principe}
	On a vu qu'en utilisant des patrons, on pouvait ramener la structure des entrée (les mots) à des features.

	\begin{table}
		\begin{tabu} to .9\textwidth {cccc}
			\mintinline{text}{[0, 0]}	& \mintinline{text}{[-1, 0]}	& \mintinline{text}{[1, 0]}	& label\\
			\midrule
			petit	& Le	& chat	& ADJ\\
			Le	& \mintinline{text}{<s>}	& petit	& DET\\
			content	& chat	& \mintinline{text}{</s>}	& ADJ\\
			chat	& petit	& est	& N
		\end{tabu}
	\end{table}

	À partir de là, on peut simplement prédire les étiquettes avec un classifieur.
\end{frame}

\begin{frame}{Avec Naïve Bayes}
	\textbf{Rappel} Avec Naïve Bayes on classifie un exemple $X$ dans la classe $C$ qui maximise $s(C,X)$ avec
	\begin{equation}
		s(C,X) = P(C) × P(x₁|C) × … × P(x_n|C)
	\end{equation}
	où les $xᵢ$ sont les features de $X$.

	Par exemple pour notre exemple $X = (\text{petit}, \text{Le}, \text{chat})$
	\begin{equation}
		\begin{split}
			s(\text{ADJ}, X)
				&= P(\text{ADJ})\\
				&× P([0,0]=\text{petit}|\text{ADJ})\\
				&× P([-1,0]=\text{Le}|\text{ADJ})\\
				&× P([1,0]=\text{chat}|\text{ADJ})
		\end{split}
	\end{equation}
\end{frame}

\begin{frame}{Apprentissage et inférence}
	\textbf{Apprentissage} À partir d'un corpus d'entraînement, on peut estimer les probabilités avec des Statistiques basiques.

	\textbf{Inférence}
	\begin{itemize}
		\item On considère chaque mot à annoter indépendamment
		\item On affecte à chacun l'étiquette qui a le meilleur score
	\end{itemize}

	\begin{itemize}
		\item Avantage : c'est très simple et très rapide
		\item Inconvénient : c'est souvent un peu trop idiot
	\end{itemize}
\end{frame}

\begin{frame}{Les problèmes}
	Outre les défauts habituels de Naïve Bayes, le plus gros problème :

	\vspace{\bigskipamount}
	\begin{overprint}
		\onslide<2>
			\begin{center}
				\Huge
				\textbf{On oublie la structuration des étiquettes}
			\end{center}
		\onslide<3->
			\begin{center}
				\textbf{On oublie la structuration des étiquettes}
			\end{center}

			Par exemple, comment étiqueter la séquence « Le coqui » si on a jamais rencontré ce mot ?

			\visible<4>{
				Si on sait qu'un déterminant est souvent suivi par un nom, on peut raisonnablement faire l'hypothèse que \emph{coqui} est un nom\only<4>{\footnote{\tiny Et on aura raison : \emph{eleutherodactylus coqui} est une charmante grenouille arboricole portoricaine.}}.
			}
	\end{overprint}
\end{frame}

\subsection{Modèles de Markov Cachés}
\begin{frame}{HMM}
	La façon la plus simple de tenir compte de la structuration des étiquettes : utiliser un \alert{\textit{\textbf{H}idden \textbf{M}arkov \textbf{M}odel}}.

	Moralement, il s'agit d'ajouter au modèle Naïve Bayes qu'on vient de voir une dépendance entre étiquettes.
\end{frame}

\begin{frame}{Formalisme}
	Pour simplifier, on va considérer qu'on utilise les formes comme seules features.

	\begin{itemize}
		\item On considère la séquence de mots : $X=(x₁, …, x_n)$
			\begin{itemize}
				\item[→] (Le, petit, chat, est, content)
			\end{itemize}
		\item On cherche à déterminer la séquence d'étiquettes $Y=(y₁, …, y_n)$ qui maximise la probabilité $P(Y|X)$
			\begin{itemize}
				\item[→] (DET, ADJ, N, V, ADJ)
			\end{itemize}
	\end{itemize}

	La différence avec précédemment c'est qu'ici on optimise sur toute la séquence et pas mot par mot.

	\pause

	D'après le théorème de Bayes, on a
	\begin{equation}
		P(Y|X) = \frac{P(X|Y)×P(Y)}{P(X)}
	\end{equation}

	Comme pour Naïve Bayes, il suffit de maximiser $P(X|Y)×P(Y)$.
\end{frame}

\begin{frame}{Probabilité d'émission}
	On appelle $P(X|Y)$ \alert{probabilité d'émission}.

	Comme pour Naïve Bayes, comme il est peu probable qu'on ait rencontré suffisamment de fois $X$ dans notre corpus pour pouvoir l'estimer, on va devoir faire une hypothèse d'indépendance en supposant que
	\begin{equation}
		P(X|Y) = P(x₁|y₁) × … × P(x_n|y_n)
	\end{equation}
	soit pour notre exemple
	\begin{equation}
		P(\text{Le}, …, \text{content}|\text{DET}, …, \text{ADJ}) = P(\text{Le}|\text{DET}) × … × P(\text{content}|\text{ADJ})
	\end{equation}
\end{frame}

\begin{frame}{Probabilité de transition}
	On appelle $P(Y)$ la \alert{probabilité de transition}.
	Il s'agit de la probabilité de la séquence d'étiquettes indépendamment du contexte.
	Dans notre exemple
	\begin{equation}
		P(Y) = P(DET, ADJ, N, V, ADJ)
	\end{equation}

	Là encore, il est en général peu probable qu'on l'ait rencontrée assez de fois pour estimer sa probabilité, mais supposer l'indépendance nous fait perdre l'information qu'on voulait conserver : \alert{la structuration}.
\end{frame}

% TODO: frame pour expliquer
% P(Y)
% 	&= P(y₁)×P((yᵢ)_{i>1}|y₁)
% 	&= P(y₁)×P(y₂|y₁)×P((yᵢ)_{i>2}|y₁,y₂)
% 	…

% TODO: figure
\begin{frame}{Hypothèse de Markov}
	Pour permettre d'estimer $P(Y)$, on va supposer que la probabilité de chaque étiquette ne dépend que de celle qui précède, ce qui donne\footnote{Avec une notation abominable.}
	\begin{equation}
		P(Y) = P(y₁)×P(y₂|y₁)×P(y₃|y₂)×…×P(y_{n-1}|y_n)
	\end{equation}
	Or on sait estimer $P(y|y')$ : il suffit de compter dans le corpus le nombre d'occurrences de $(y',y)$ et de $y'$ et d'appliquer la définition.
\end{frame}

\begin{frame}{Inférence}
	Les simplifications précédentes permettent d'apprendre facilement le modèle : il reste à le mettre en action.

	Sous ces hypothèses, il existe plusieurs façon de déterminer une séquence d'étiquettes probables pour une séquence de mots

	Des méthodes simples et inexactes
	\begin{itemize}
		\item Heuristique gloutonne
		\item \emph{Beam search}
	\end{itemize}

	Une méthode un peu plus sophistiquée (et plus lente) mais exacte : l'algorithme de Viterbi.
\end{frame}

\begin{frame}[standout]
	La suite au tableau
\end{frame}

\begin{frame}{Features}
	On peut bien sûr améliorer ce modèle en prenant en compte plus de features que simplement les formes.

	En supposant encore que les features sont indépendantes, ça revient à considérer des probabilités d'émission composites :

	\begin{equation}
		\begin{split}
			P([0,0]{=}\text{petit}&, [-1,0]{=}\text{Le}|ADJ) =\\
				& P([0,0]{=}\text{petit}|ADJ) × P([-1,0]{=}\text{Le}|ADJ)
		\end{split}
	\end{equation}

	Cependant ce n'est pas le cadre le plus sympathique pour travailler.
	\begin{itemize}
		\item L'hypothèse d'indépendance est vite gênante
		\item[→] On aimerait pouvoir utiliser des features composites
	\end{itemize}
\end{frame}

\begin{frame}{HMM}
	On peut penser aux modèles de Markov cachés comme un équivalent de Naïve Bayes pour l'étiquetage, avec les mêmes avantages
	\begin{itemize}
		\item Faciles et rapides pour l'entraînement et l'inférence
		\item Étonnamment efficace même pour peu de données
	\end{itemize}
	Et les mêmes inconvénients
	\begin{itemize}
		\item Interprétabilité limitée avec beaucop de features
		\item Les hypothèses d'indépendance simplifient parfois trop le problème.
	\end{itemize}
\end{frame}

\subsection{MEMM et CRF}
\begin{frame}{MEMM}
	Les \alert{\textit{\textbf{M}aximum \textbf{E}ntropy \textbf{M}arkov \textbf{M}odels}} sont une reformulation des HMM où on modélise directement la probabilité $P(Y|X)$ en définissant manuellement ses composantes.

	Comme pour les HMM, on travaille sous l'hypothèse que les étiquettes sont émises suivant un processus Markovien, c'est à dire qu'une étiquette ne dépend directement que de l'étiquette précédente.
\end{frame}

\begin{frame}{Forme du modèle}
	On écrit comme précédemment

	\begin{equation}
		P(y₁,…y_n|X) = ∏P(yᵢ|y₁,…, y_{i-1}, X)
	\end{equation}

	Soit, avec l'hypothèse de \alert{Markov}

	\begin{equation}
		P(y₁,…y_n|X) = ∏P(yᵢ|y_{i-1}, X)
	\end{equation}
\end{frame}

\begin{frame}{Forme des probabilités}
	On modélise les probabilités comme
	\begin{equation}
		P(yᵢ|y_{i-1}, X) = \frac{e^{ϕ(yᵢ, y_{i-1}, X)}}{∑_c e^{ϕ(c, y_{i-1}, X)}}
	\end{equation}
	Avec
	\begin{equation}
		ϕ(yᵢ, y_{i-1}, X) = \sum_k w_k  ϕ_k(yᵢ, y_{i-1}, X)
	\end{equation}
	Où
	\begin{itemize}
		\item Les $ϕ_k$ sont des features définies manuellement (souvent binaires)
		\item Les $w_k$ sont des poids appris
	\end{itemize}

	On parle de modèle \alert{log-linéaire}.
\end{frame}

\begin{frame}{Inférence}
	L'inférence avec un MEMM se fait exactement comme avec un HMM : on a des probas de transition, donc on peut soit appliquer une heuristique rapide soit Viterbi.

	L'apprentissage, c'est une autre paire de manches…
\end{frame}

% Plus facile d'ajouter des features
% Plus coûteux à entraîner
% Inférence : Viterbi ou (rarement) beam search
% Différence : MEMM normalise sur les états, CRF sur les séquences
% Csq : e.g. CRF capable de passer par une transition de proba 0 si
% elle donne une grosse proba pour la suite

%  █████  ██████  ██████  ███████ ███    ██ ██████  ██ ██   ██
% ██   ██ ██   ██ ██   ██ ██      ████   ██ ██   ██ ██  ██ ██
% ███████ ██████  ██████  █████   ██ ██  ██ ██   ██ ██   ███
% ██   ██ ██      ██      ██      ██  ██ ██ ██   ██ ██  ██ ██
% ██   ██ ██      ██      ███████ ██   ████ ██████  ██ ██   ██

\appendix
\pgfkeys{/metropolis/inner/sectionpage=simple}  % Avoid random errors with section page progressbar
\section{Annexes}
\pdfbookmark[2]{Références}{references}
\begin{frame}[allowframebreaks]{References}
    \printbibliography[heading=none]
\end{frame}

\pdfbookmark[2]{Remerciements}{acknowledgements}
\begin{frame}{Remerciements}
    Ce cours a été construit à partir du polycopié de cours \citetitle{tellier2017IntroductionFouilleTextes} \parencite{tellier2017IntroductionFouilleTextes} et des précieux conseils d'Isabelle Tellier que je ne saurais trop remercier pour sa confiance et son dévouement.
\end{frame}

\pdfbookmark[2]{Licence}{licence}
\begin{frame}{Licence}
    \begin{center}
        {\huge \ccby}
        \vfill
        This document is available under the terms of the Creative Commons Attribution 4.0 International License (CC BY 4.0) (\shorturl{creativecommons.org/licenses/by/4.0})

        Exceptions to the above statement are listed at {\small\shorturl{loicgrobol.github.io/intro-fouille-textes\#licences}}
        \vfill
        © 2019, Loïc Grobol <\shorturl[mailto][:]{loic.grobol@gmail.com}>

        \shorturl[http]{lattice.cnrs.fr/Grobol-Loic}
    \end{center}
\end{frame}

\end{document}
