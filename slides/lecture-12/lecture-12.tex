% LTeX: language=fr
% Copyright © 2019, Loïc Grobol <loic.grobol@gmail.com>
% This document is available under the terms of the Creative Commons Attribution 4.0 International License (CC BY 4.0) (https://creativecommons.org/licenses/by/4.0/)

\documentclass[../allslides.tex]{subfiles}

\nocite{
	collinsLogLinearModelsMEMMs,
	neubig2019TutorialProgrammingNatural,
	jurafsky2019SpeechLanguageProcessing,
	lafferty2001ConditionalRandomFields
}

\begin{document}
\renewcommand\titlepagesubtitle
\renewcommand\docdate{2021-02-18}  % chktex 8

\ExplSyntaxOn
    \DeclareExpandableDocumentCommand\eval{m}{\fp_eval:n{#1}}
\ExplSyntaxOff
\lecture{Modèles d'étiquetage}{Cours 12}


\begin{frame}[fragile=singleslide]{Rappel}
	\begin{quote}
		L'\emph{annotation} ou \emph{étiquetage} est la tâche consistant à attribuer une étiquette à chaque élément d'un ensemble \alert{structuré}
	\end{quote}
	\begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}
            \matrix[matrix of nodes, anchor=base, row sep=1.5em] (m) {
                Le  & petit & chat  & est & content\\
                DET & ADJ   & NC    & V   & ADJ\\
            };
            \foreach \x in {1, ..., 5}
                \draw[->] (m-1-\x) -- (m-2-\x);
        \end{tikzpicture}
        \caption{\emph{Part Of Speech Tagging}}
    \end{figure}
	Ici on parlera essentiellement de l'étiquetage de \alert{séquences}.
\end{frame}

% TODO: autres tâches (traduction, normalisation des tweets…)

\section{Représenter les données}
\begin{frame}[fragile=singleslide]{Formats}
	On a une tâche de fouille de textes à réaliser

	\begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}
            \matrix[matrix of nodes, anchor=base, row sep=1.5em] (m) {
                Le  & petit & chat  & est & content\\
                DET & ADJ   & NC    & V   & ADJ\\
            };
            \foreach \x in {1, ..., 5}
                \draw[->] (m-1-\x) -- (m-2-\x);
        \end{tikzpicture}
        \caption{\emph{Part Of Speech Tagging}}
    \end{figure}

	La première chose à faire est de représenter les données du problèmes sous forme lisible par la machine.
\end{frame}

\begin{frame}[fragile]{Format CoNLL}
	\begin{columns}
		\column{0.45\textwidth}
			\begin{figure}
				\begin{minted}{text}
					Le	DET
					petit	ADJ
					chat	N
					est	V
					content	ADJ

					Vive	V
					le	DET
					TAL	N
				\end{minted}
			\end{figure}
		\column{0.45\textwidth}
			\begin{itemize}
				\item Un mot par ligne
				\item Colonnes séparées par des tabulateurs
				\item \alert{Séquences séparées par des lignes vides}
			\end{itemize}
	\end{columns}
	Est-ce un format tabulaire au sens habituel ?
\end{frame}

\begin{frame}[fragile]{Indépendance des lignes}
	% This because of the fragility of pygments' parser
	\tikzset{
		squarearound/.style={draw={#1}, line width=2pt, inner sep=3pt},
	}
	\begin{columns}
		\column{0.45\textwidth}
			\begin{figure}
				\begin{minted}[escapeinside=||]{text}
					Le	DET
					petit	|\textnode[squarearound=highlight0]{petitadj}{ADJ}|
					chat	N
					est	V
					content	ADJ
				\end{minted}
			\end{figure}
		\column{0.45\textwidth}
			\begin{figure}
				\begin{minted}[escapeinside=||]{text}
					Le	DET
					petit	|\textnode[squarearound=highlight1]{petitn}{N}|
					mange	V
					la	DET
					crêpe	N
				\end{minted}
			\end{figure}
	\end{columns}
\end{frame}

\begin{frame}{Du format}
	Ce \alert{n'est pas} un format tabulaire au sens précédent de ce cours.

	\begin{itemize}
		\item Les lignes ne sont pas indépendantes
		\item L'ordre des lignes compte
	\end{itemize}

	En revanche
	\begin{itemize}
		\item Les \alert{blocs} sont indépendants
		\item Leur ordre n'est pas significatif
	\end{itemize}

	Moralment, les blocs sont donc ici l'équivalent des lignes des fichiers tabulaires
\end{frame}

\begin{frame}[fragile]{Contexte}
	La non-indépendances des lignes d'un même bloc fait que pour prédire une étiquette, il vaut tenir compte du \alert{contexte}.

	\begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}[
			faded/.style={black!30},
		]
			\matrix (mat) [
				matrix of nodes,
				column sep=2em,
				row sep=1em,
				nodes={align=left, anchor=base west},
			]{
				|[alt=<-2>{faded}{}]| Le & |[faded]| DET\\
				petit & \alt<1>{?}{ADJ}\\
				|[alt=<1>{faded}{}]| chat & |[faded]| N\\
				|[alt=<-2>{faded}{}]| est & |[faded]| V\\
				|[alt=<-2>{faded}{}]| content & |[faded]| ADJ\\
			};
			\draw[->] (mat-2-1) -- (mat-2-2);
			\draw[->, visible on={2-}] (mat-3-1) -- (mat-2-2);
			\foreach \i in {1,4,5}
				\draw[->, visible on=3] (mat-\i-1) -- (mat-2-2);
		\end{tikzpicture}
	\end{figure}
	\only<3>{Mais l'intégralité du contexte n'est pas toujours utile.}
\end{frame}

\begin{frame}[fragile]{Contexte}
	En général on ne tient compte pour chaque étiquette que d'une partie du contexte.

	Par exemple le mot d'avant et le mot d'après
	\begin{figure}
        \tikzset{external/export=true}
        \begin{tikzpicture}[
			faded/.style={black!30},
		]
			\matrix (mat) [
				matrix of nodes,
				row sep=1em,
				column sep=2em,
				column 1/.style={align=right, anchor=base east},
				column 2/.style={align=left, anchor=base west},
			]{
				Le &  DET\\
				petit & ADJ\\
				chat &  N\\
				$⋮$ & $⋮$\\
			};
			\draw[->, visible on=1] (mat-1-1) -- (mat-1-2);
			\draw[->, visible on=1] (mat-2-1) -- (mat-1-2);

			\draw[->, visible on=2] (mat-1-1) -- (mat-2-2);
			\draw[->, visible on=2] (mat-2-1) -- (mat-2-2);
			\draw[->, visible on=2] (mat-3-1) -- (mat-2-2);

			\draw[->, visible on=3] (mat-2-1) -- (mat-3-2);
			\draw[->, visible on=3] (mat-3-1) -- (mat-3-2);
		\end{tikzpicture}
	\end{figure}
	\only<3>{}
\end{frame}

\begin{frame}[fragile=singleslide]{Features contextuelles}
	D'un point de vue apprentissage, « tenir compte du contexte » signifie utiliser les mots du contexte comme \alert{features}.

	Dans un format tabulaire, les dépendances considérées donnent donc
	\begin{table}
		\begin{tabu} to .9\textwidth {cccc}
			[0]	& [-1]	& [1]	& label\\
			\midrule
			Le	& \mintinline{text}{<s>}	& petit	& DET\\
			petit	& Le	& chat	& ADJ\\
			chat	& petit	& est	& N\\
			content	& chat	& \mintinline{text}{</s>}	& ADJ
		\end{tabu}
	\end{table}

	\begin{itemize}
		\item Les lignes sont bien indépendantes
		\item Les features sont des position relative : pour une étiquette donnée, le mot précédent a l'indice $-1$, le mot à étiqueter $0$, le mot suivant $1$ et ainsi de suite.
	\end{itemize}
\end{frame}

\begin{frame}[fragile=singleslide]{Patrons de features}
	On ne se limite pas aux formes pour déterminer une étiquette
	\vspace{-\smallskipamount}
	\begin{table}
		\begin{tabu} to .9\textwidth {ccc}
			Les	& le	& DET\\
			enfants	& enfant	& N\\
			sont	& être	& V\\
			pénibles & pénible	& ADJ
		\end{tabu}
	\end{table}
	\vspace{-\smallskipamount}
	Dans ce cas les features sont précisées en deux dimensions :
	% FIXME: ce tableau est faux
	\begin{table}
		\begin{tabu} to .9\textwidth {ccc}
			\mintinline{text}{[0, 0]}	& \mintinline{text}{[1, 1]}	& label\\
			\midrule
			Les	& enfant 	& DET\\
			enfants	& être 	& ADJ\\
			sont	& pénible 	& N\\
			pénibles	& \mintinline{text}{</s>}	& ADJ
		\end{tabu}
	\end{table}
	\vspace{-1.5\smallskipamount}
	Ici \mintinline{text}{[1, 1]} désigne la valeur de la colonne $1$ pour la ligne suivant l'étiquette.
\end{frame}


\section{Modèles d'étiquetage}
\subsection{Classification indépendante}
\begin{frame}[fragile]{Principe}
	On a vu qu'en utilisant des patrons, on pouvait ramener la structure des entrée (les mots) à des features.

	\begin{table}
		\begin{tabu} to .9\textwidth {cccc}
			\mintinline{text}{[0, 0]}	& \mintinline{text}{[-1, 0]}	& \mintinline{text}{[1, 0]}	& label\\
			\midrule
			petit	& Le	& chat	& ADJ\\
			Le	& \mintinline{text}{<s>}	& petit	& DET\\
			content	& chat	& \mintinline{text}{</s>}	& ADJ\\
			chat	& petit	& est	& N
		\end{tabu}
	\end{table}

	À partir de là, on peut simplement prédire les étiquettes avec un classifieur.
\end{frame}

\begin{frame}{Avec Naïve Bayes}
	\textbf{Rappel} Avec Naïve Bayes on classifie un exemple $X$ dans la classe $C$ qui maximise $s(C,X)$ avec
	\begin{equation}
		s(C,X) = P(C) × P(x₁|C) × … × P(x_n|C)
	\end{equation}
	où les $xᵢ$ sont les features de $X$.

	Par exemple pour notre exemple $X = (\text{petit}, \text{Le}, \text{chat})$
	\begin{equation}
		\begin{split}
			s(\text{ADJ}, X)
				&= P(\text{ADJ})\\
				&× P([0,0]=\text{petit}|\text{ADJ})\\
				&× P([-1,0]=\text{Le}|\text{ADJ})\\
				&× P([1,0]=\text{chat}|\text{ADJ})
		\end{split}
	\end{equation}
\end{frame}

\begin{frame}{Apprentissage et inférence}
	\textbf{Apprentissage} À partir d'un corpus d'entraînement, on peut estimer les probabilités avec des Statistiques basiques.

	\textbf{Inférence}
	\begin{itemize}
		\item On considère chaque mot à annoter indépendamment
		\item On affecte à chacun l'étiquette qui a le meilleur score
	\end{itemize}

	\begin{itemize}
		\item Avantage : c'est très simple et très rapide
		\item Inconvénient : c'est souvent un peu trop idiot
	\end{itemize}
\end{frame}

\begin{frame}{Les problèmes}
	Outre les défauts habituels de Naïve Bayes, le plus gros problème :

	\vspace{\bigskipamount}
	\begin{overprint}
		\onslide<2>
			\begin{center}
				\Huge
				\textbf{On oublie la structuration des étiquettes}
			\end{center}
		\onslide<3->
			\begin{center}
				\textbf{On oublie la structuration des étiquettes}
			\end{center}

			Par exemple, comment étiqueter la séquence « Le coqui » si on a jamais rencontré ce mot ?

			\visible<4>{
				Si on sait qu'un déterminant est souvent suivi par un nom, on peut raisonnablement faire l'hypothèse que \emph{coqui} est un nom\only<4>{\footnote{\tiny Et on aura raison : \emph{eleutherodactylus coqui} est une charmante grenouille arboricole portoricaine.}}.
			}
	\end{overprint}
\end{frame}

\subsection{Modèles de Markov Cachés}
\begin{frame}{HMM}
	La façon la plus simple de tenir compte de la structuration des étiquettes : utiliser un \alert{\textit{\textbf{H}idden \textbf{M}arkov \textbf{M}odel}}.

	Moralement, il s'agit d'ajouter au modèle Naïve Bayes qu'on vient de voir une dépendance entre étiquettes.
\end{frame}

\begin{frame}{Formalisme}
	Pour simplifier, on va considérer qu'on utilise les formes comme seules features.

	\begin{itemize}
		\item On considère la séquence de mots : $X=(x₁, …, x_n)$
			\begin{itemize}
				\item[→] (Le, petit, chat, est, content)
			\end{itemize}
		\item On cherche à déterminer la séquence d'étiquettes $Y=(y₁, …, y_n)$ qui maximise la probabilité $P(Y|X)$
			\begin{itemize}
				\item[→] (DET, ADJ, N, V, ADJ)
			\end{itemize}
	\end{itemize}

	La différence avec précédemment c'est qu'ici on optimise sur toute la séquence et pas mot par mot.

	\pause

	D'après le théorème de Bayes, on a
	\begin{equation}
		P(Y|X) = \frac{P(X|Y)×P(Y)}{P(X)}
	\end{equation}

	Comme pour Naïve Bayes, il suffit de maximiser $P(X|Y)×P(Y)$.
\end{frame}

\begin{frame}{Probabilité d'émission}
	On appelle $P(X|Y)$ \alert{probabilité d'émission}.

	Comme pour Naïve Bayes, comme il est peu probable qu'on ait rencontré suffisamment de fois $X$ dans notre corpus pour pouvoir l'estimer, on va devoir faire une hypothèse d'indépendance en supposant que
	\begin{equation}
		P(X|Y) = P(x₁|y₁) × … × P(x_n|y_n)
	\end{equation}
	soit pour notre exemple
	\begin{equation}
		P(\text{Le}, …, \text{content}|\text{DET}, …, \text{ADJ}) = P(\text{Le}|\text{DET}) × … × P(\text{content}|\text{ADJ})
	\end{equation}
\end{frame}

\begin{frame}{Probabilité de transition}
	On appelle $P(Y)$ la \alert{probabilité de transition}.
	Il s'agit de la probabilité de la séquence d'étiquettes indépendamment du contexte.
	Dans notre exemple
	\begin{equation}
		P(Y) = P(DET, ADJ, N, V, ADJ)
	\end{equation}

	Là encore, il est en général peu probable qu'on l'ait rencontrée assez de fois pour estimer sa probabilité, mais supposer l'indépendance nous fait perdre l'information qu'on voulait conserver : \alert{la structuration}.
\end{frame}

% TODO: frame pour expliquer
% P(Y)
% 	&= P(y₁)×P((yᵢ)_{i>1}|y₁)
% 	&= P(y₁)×P(y₂|y₁)×P((yᵢ)_{i>2}|y₁,y₂)
% 	…

% TODO: figure
\begin{frame}{Hypothèse de Markov}
	Pour permettre d'estimer $P(Y)$, on va supposer que la probabilité de chaque étiquette ne dépend que de celle qui précède, ce qui donne\footnote{Avec une notation abominable.}
	\begin{equation}
		P(Y) = P(y₁)×P(y₂|y₁)×P(y₃|y₂)×…×P(y_{n-1}|y_n)
	\end{equation}
	Or on sait estimer $P(y|y')$ : il suffit de compter dans le corpus le nombre d'occurrences de $(y',y)$ et de $y'$ et d'appliquer la définition.
\end{frame}

\begin{frame}{Inférence}
	Les simplifications précédentes permettent d'apprendre facilement le modèle : il reste à le mettre en action.

	Sous ces hypothèses, il existe plusieurs façon de déterminer une séquence d'étiquettes probables pour une séquence de mots

	Des méthodes simples et inexactes
	\begin{itemize}
		\item Heuristique gloutonne
		\item \emph{Beam search}
	\end{itemize}

	Une méthode un peu plus sophistiquée (et plus lente) mais exacte : l'algorithme de Viterbi.
\end{frame}

% TODO: Lattice figure
% TODO: Viterbi animation

\begin{frame}[standout]
	La suite au tableau
\end{frame}

\begin{frame}{Features}
	On peut bien sûr améliorer ce modèle en prenant en compte plus de features que simplement les formes.

	En supposant encore que les features sont indépendantes, ça revient à considérer des probabilités d'émission composites :

	\begin{equation}
		\begin{split}
			P([0,0]{=}\text{petit}&, [-1,0]{=}\text{Le}|ADJ) =\\
				& P([0,0]{=}\text{petit}|ADJ) × P([-1,0]{=}\text{Le}|ADJ)
		\end{split}
	\end{equation}

	Cependant ce n'est pas le cadre le plus sympathique pour travailler.
	\begin{itemize}
		\item L'hypothèse d'indépendance est vite gênante
		\item[→] On aimerait pouvoir utiliser des features composites
	\end{itemize}
\end{frame}

\begin{frame}{HMM}
	On peut penser aux modèles de Markov cachés comme un équivalent de Naïve Bayes pour l'étiquetage, avec les mêmes avantages
	\begin{itemize}
		\item Faciles et rapides pour l'entraînement et l'inférence
		\item Étonnamment efficace même pour peu de données
	\end{itemize}
	Et les mêmes inconvénients
	\begin{itemize}
		\item Interprétabilité limitée avec beaucoup de features
		\item Les hypothèses d'indépendance simplifient parfois trop le problème.
	\end{itemize}
\end{frame}

\subsection{MEMM et CRF}
\begin{frame}{MEMM}
	Les \alert{\textit{\textbf{M}aximum \textbf{E}ntropy \textbf{M}arkov \textbf{M}odels}} sont une reformulation des HMM où on modélise directement la probabilité $P(Y|X)$ en définissant manuellement ses composantes.

	Comme pour les HMM, on travaille sous l'hypothèse que les étiquettes sont émises suivant un processus Markovien, c'est à dire qu'une étiquette ne dépend directement que de l'étiquette précédente.
\end{frame}

\begin{frame}{Forme du modèle}
	On écrit comme précédemment

	\begin{equation}
		P(y₁,…y_n|X) = ∏P(yᵢ|y₁,…, y_{i-1}, X)
	\end{equation}

	Soit, avec l'hypothèse de \alert{Markov}

	\begin{equation}
		P(y₁,…y_n|X) = ∏P(yᵢ|y_{i-1}, X)
	\end{equation}
\end{frame}

\begin{frame}{Forme des probabilités}
	On modélise les probabilités comme
	\begin{equation}
		P(yᵢ|y_{i-1}, X) = \frac{e^{ϕ(yᵢ, y_{i-1}, X)}}{∑_c e^{ϕ(c, y_{i-1}, X)}}
	\end{equation}
	Avec
	\begin{equation}
		ϕ(yᵢ, y_{i-1}, X) = \sum_k w_k  ϕ_k(yᵢ, y_{i-1}, X)
	\end{equation}
	Où
	\begin{itemize}
		\item Les $ϕ_k$ sont des features définies manuellement (souvent binaires)
		\item Les $w_k$ sont des poids appris
	\end{itemize}

	On parle de modèle \alert{log-linéaire}.
\end{frame}

\begin{frame}{Inférence}
	L'inférence avec un MEMM se fait exactement comme avec un HMM : on a des probas de transition, donc on peut soit appliquer une heuristique rapide soit Viterbi.

	L'apprentissage, c'est une autre paire de manches…
\end{frame}

% Plus facile d'ajouter des features
% Plus coûteux à entraîner
% Inférence : Viterbi ou (rarement) beam search
% Différence : MEMM normalise sur les états, CRF sur les séquences
% Csq : e.g. CRF capable de passer par une transition de proba 0 si
% elle donne une grosse proba pour la suite

%  █████  ██████  ██████  ███████ ███    ██ ██████  ██ ██   ██
% ██   ██ ██   ██ ██   ██ ██      ████   ██ ██   ██ ██  ██ ██
% ███████ ██████  ██████  █████   ██ ██  ██ ██   ██ ██   ███
% ██   ██ ██      ██      ██      ██  ██ ██ ██   ██ ██  ██ ██
% ██   ██ ██      ██      ███████ ██   ████ ██████  ██ ██   ██

\ifSubfilesClassLoaded{
	\appendix
	\pgfkeys{/metropolis/inner/sectionpage=simple}  % Avoid random errors with section page progressbar
	\section{Annexes}
	\pdfbookmark[2]{Remerciements}{acknowledgements}
	\begin{frame}{Remerciements}
		Ce cours a été construit à partir du polycopié de cours \citetitle{tellier2017IntroductionFouilleTextes} \parencite{tellier2017IntroductionFouilleTextes} et des précieux conseils d'Isabelle Tellier que je ne saurais trop remercier pour sa confiance et son dévouement.
	\end{frame}

	\pdfbookmark[2]{Références}{references}
	\begin{frame}[allowframebreaks]{References}
		\printbibliography[heading=none]
	\end{frame}

	\pdfbookmark[2]{Licence}{licence}
	\begin{frame}{Licence}
		\begin{center}
			{\huge \ccby}
			\vfill
			This document is available under the terms of the Creative Commons Attribution 4.0 International License (CC BY 4.0) (\shorturl{creativecommons.org/licenses/by/4.0})

			Exceptions to the above statement are listed at {\small\shorturl{loicgrobol.github.io/intro-fouille-textes\#licences}}
			\vfill
			© 2019, Loïc Grobol <\shorturl[mailto][:]{loic.grobol@gmail.com}>

			\shorturl[http]{lattice.cnrs.fr/Grobol-Loic}
		\end{center}
	\end{frame}
}{}

\end{document}
