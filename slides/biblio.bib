
@inproceedings{alishahi2017EncodingPhonologyRecurrent,
  title = {Encoding of phonology in a recurrent neural model of grounded speech},
  booktitle = {Proceedings of the 21st {{Conference}} on {{Computational Natural Language Learning}}},
  author = {Alishahi, Afra and Barking, Marie and Chrupała, Grzegorz},
  date = {2017},
  pages = {368--378},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/K17-1037},
  url = {http://www.aclweb.org/anthology/K17-1037},
  eventtitle = {{{CoNLL}} 2017},
  venue = {Vancouver, Canada}
}

@book{amini2013RechercheInformationApplications,
  title = {Recherche d'{{Information}} - {{Applications}}, modèles et algorithmes},
  author = {Amini, Massih-Reza and Gaussier, Éric},
  date = {2013-04},
  publisher = {{Eyrolles}},
  url = {https://hal.archives-ouvertes.fr/hal-00881257},
  urldate = {2018-01-13},
  hal_id = {hal-00881257},
  hal_version = {v1},
  keywords = {\#nosource,Big Data,Classification documentaire,Clustering,Moteurs de recherche,Recherche d'Information}
}

@online{belkin2018ReconcilingModernMachine,
  title = {Reconciling modern machine learning practice and the bias-variance trade-off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  date = {2018-12-28},
  url = {http://arxiv.org/abs/1812.11118},
  urldate = {2019-09-27},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners.},
  archiveprefix = {arXiv},
  eprint = {1812.11118},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryclass = {cs, stat}
}

@inproceedings{bergstra2011AlgorithmsHyperParameterOptimization,
  title = {Algorithms for {{Hyper}}-{{Parameter Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bergstra, James S. and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  date = {2011},
  volume = {24},
  pages = {2546--2554},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
  urldate = {2019-10-06},
  eventtitle = {{{NeurIPS}} 2011},
  venue = {Granada, España}
}

@article{cardon2018RevancheNeurones,
  title = {La revanche des neurones},
  author = {Cardon, Dominique and Cointet, Jean-Philippe and Mazières, Antoine},
  date = {2018-11-16},
  journaltitle = {Reseaux},
  volume = {n° 211},
  pages = {173--220},
  issn = {0751-7971},
  url = {https://www.cairn.info/revue-reseaux-2018-5-page-173.htm},
  urldate = {2019-03-05},
  abstract = {Depuis 2010, les techniques prédictives basées sur l’apprentissage artificiel (machine learning), et plus spécifiquement des réseaux de neurones (deep learning), réalisent des prouesses spectaculaires dans les domaines de la reconnaissance d’image ou de la traduction automatique, sous l’égide du terme d’“Intelligence artificielle”. Or l’appartenance de ces techniques à ce domaine de recherche n’a pas toujours été de soi. Dans l’histoire tumultueuse de l’IA, les techniques d’apprentissage utilisant des réseaux de neurones – que l’on qualifie de “connexionnistes” – ont même longtemps été moquées et ostracisées par le courant dit “symbolique”. Cet article propose de retracer l’histoire de l’Intelligence artificielle au prisme de la tension entre ces deux approches, symbolique et connexionniste. Dans une perspective d’histoire sociale des sciences et des techniques, il s’attache à mettre en évidence la manière dont les chercheurs, s’appuyant sur l’arrivée de données massives et la démultiplication des capacités de calcul, ont entrepris de reformuler le projet de l’IA symbolique en renouant avec l’esprit des machines adaptatives et inductives de l’époque de la cybernétique.},
  langid = {french},
  number = {5}
}

@article{cheng2019LearningExecutableNeural,
  title = {Learning an {{Executable Neural Semantic Parser}}},
  author = {Cheng, Jianpeng and Reddy, Siva and Saraswat, Vijay and Lapata, Mirella},
  date = {2019-03},
  journaltitle = {Computational Linguistics},
  volume = {45},
  pages = {59--94},
  doi = {10.1162/coli_a_00342},
  url = {https://www.aclweb.org/anthology/J19-1002},
  urldate = {2020-02-25},
  abstract = {This article describes a neural semantic parser that maps natural language utterances onto logical forms that can be executed against a task-specific environment, such as a knowledge base or a database, to produce a response. The parser generates tree-structured logical forms with a transition-based approach, combining a generic tree-generation algorithm with domain-general grammar defined by the logical language. The generation process is modeled by structured recurrent neural networks, which provide a rich encoding of the sentential context and generation history for making predictions. To tackle mismatches between natural language and logical form tokens, various attention mechanisms are explored. Finally, we consider different training settings for the neural semantic parser, including fully supervised training where annotated logical forms are given, weakly supervised training where denotations are provided, and distant supervision where only unlabeled sentences and a knowledge base are available. Experiments across a wide range of data sets demonstrate the effectiveness of our parser.},
  number = {1}
}

@article{church2011PendulumSwungToo,
  title = {A pendulum swung too far},
  author = {Church, Kenneth},
  date = {2011-10},
  journaltitle = {Linguistic Issues in Language Technology},
  volume = {6},
  url = {https://journals.linguisticsociety.org/elanguage/lilt/article/view/2581.html}
}

@article{collins2011LogLinearModelsMEMMs,
  title = {Log-{{Linear Models}}, {{MEMMs}}, and {{CRFs}}},
  author = {Collins, Michael},
  date = {2011},
  pages = {11},
  langid = {english}
}

@book{cornuejols2010ApprentissageArtificielConcepts,
  title = {Apprentissage artificiel - {{Concepts}} et algorithmes},
  author = {Cornuéjols, Antoine and Miclet, Laurent},
  date = {2010},
  edition = {2},
  publisher = {{Eyrolles}},
  url = {https://www.eyrolles.com/Sciences/Livre/apprentissage-artificiel-9782212124712},
  urldate = {2018-01-13},
  keywords = {\#nosource}
}

@article{dice1945MeasuresAmountEcologic,
  title = {Measures of the {{Amount}} of {{Ecologic Association Between Species}}},
  author = {Dice, Lee R.},
  date = {1945},
  journaltitle = {Ecology},
  volume = {26},
  pages = {297--302},
  issn = {1939-9170},
  doi = {10.2307/1932409},
  url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.2307/1932409},
  urldate = {2019-08-22},
  langid = {english},
  number = {3}
}

@inproceedings{fort2018CheapFastGood,
  title = {Cheap, {{Fast}} and {{Good}}! {{Voting Games}} with a {{Purpose}}},
  booktitle = {{{Games4NLP}}: {{Games}} and {{Gamification}} for {{Natural Language Processing}}},
  author = {Fort, Karën and Lafourcade, Mathieu and Le Brun, Nathalie},
  date = {2018-05},
  location = {{Miyazaki, Japan}},
  url = {https://hal.archives-ouvertes.fr/hal-01790614},
  urldate = {2019-12-18},
  abstract = {We present in this paper the voting games with a purpose that were developed around JeuxDeMots, a central game aiming at creating a lexical network for French. We show that such lightweight applications can help collect quality language resources very efficiently and we advocate for a common platform for such voting games for language resources.},
  keywords = {crowdsourcing,GWAP,voting games}
}

@book{gaussier2011ModelesStatistiquesPour,
  title = {Modèles statistiques pour l'accès à l'information textuelle},
  author = {Gaussier, Éric and Yvon, François},
  date = {2011-05},
  publisher = {{Hermès / Lavoisier}},
  url = {https://hal.archives-ouvertes.fr/hal-00742830},
  urldate = {2018-01-13},
  hal_id = {hal-00742830},
  hal_version = {v1},
  keywords = {\#nosource}
}

@inproceedings{gower1998NonprobabilisticClassification,
  title = {Non-probabilistic {{Classification}}},
  booktitle = {Advances in {{Data Science}} and {{Classification}}},
  author = {Gower, John C. and Ross, Gavin J. S.},
  editor = {Rizzi, Alfredo and Vichi, Maurizio and Bock, Hans-Hermann},
  date = {1998},
  pages = {21--28},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-72253-0_3},
  abstract = {Much of the classification literature ignores notions of probability. In our view, this is due in part to a dominant tendency in the early days of computers for developing heuristic clustering algorithms and in part due to long traditions in classification outside the statistical/probabilistic orbit, of which biological taxonomy and book classification are primary examples. Statisticians have rightly stressed the role of probabilistic concepts in formulating classification problems and in interpreting classifications but we believe that they are wrong in suggesting, as they sometimes seem to, that other approaches are unsatisfactory. Probability has its proper place in classification but it is neither an essential nor always an appropriate tool. We discuss circumstances where non- probabilistically-based classifications are fully justified.Considerations influencing the differences between the two approaches include: 1) Irrespective of whether things are to be assembled into classes (arranged hierarchically or not) or assigned to previously recognised classes, methodology depends on whether the things may be regarded as representing groups or as samples from groups; 2) Models are basic to the formulation of statistically based classifications, but they may also underpin nonprobabilistic classifications; overt models are not a characteristic of heuristic classification algorithms; 3) In principle, probabilistic models allow the significance and number of clusters justified by data to be assessed. In non-probabilistic classifications (probabilistic too), the eighteenth century concept of approximation offers a good basis for assessing the adequacy and stability of clusters.},
  isbn = {978-3-642-72253-0},
  keywords = {Assignment,Class Construction,Classes,Groups,Non-probabilistic Classification,Probabilistic Classification},
  langid = {english},
  series = {Studies in {{Classification}}, {{Data Analysis}}, and {{Knowledge Organization}}}
}

@online{gwern2011NeuralNetTank,
  title = {The {{Neural Net Tank Urban Legend}} - {{Gwern}}.net},
  author = {{gwern}},
  date = {2011-09-20},
  url = {https://www.gwern.net/Tanks},
  urldate = {2019-02-21},
  abstract = {AI folklore tells a story about a neural network trained to detect tanks which instead learned to detect time of day; investigating, this probably never happened.},
  keywords = {artificial intelligence,counterexample,evolutionary algorithm,failure,machine learning},
  langid = {english}
}

@book{ibekwe-sanjuan2007FouilleTexte,
  title = {Fouille de texte},
  author = {Ibekwe-Sanjuan, Fidelia},
  date = {2007-03},
  publisher = {{Hermès / Lavoisier}},
  url = {https://hal.archives-ouvertes.fr/hal-00636094},
  urldate = {2018-01-13},
  hal_id = {hal-00636094},
  hal_version = {v1},
  keywords = {\#nosource,fouille de textes,méthodes d'agrégation,recherche d'information,veille scientifique et technologique}
}

@book{jurafsky2019SpeechLanguageProcessing,
  title = {Speech and {{Language Processing}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  date = {2019},
  edition = {3},
  publisher = {{Pearson}},
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  isbn = {978-0-13-187321-6},
  langid = {english},
  pagetotal = {1032}
}

@inproceedings{kaufman2011LeakageDataMining,
  title = {Leakage in {{Data Mining}}: {{Formulation}}, {{Detection}}, and {{Avoidance}}},
  shorttitle = {Leakage in {{Data Mining}}},
  booktitle = {Proceedings of the 17th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Kaufman, Shachar and Rosset, Saharon and Perlich, Claudia},
  date = {2011},
  pages = {556--563},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2020408.2020496},
  url = {http://doi.acm.org/10.1145/2020408.2020496},
  urldate = {2019-03-14},
  abstract = {Deemed "one of the top ten data mining mistakes", leakage is essentially the introduction of information about the data mining target, which should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, controversies around several major public data mining competitions held recently such as the INFORMS 2010 Data Mining Challenge and the IJCNN 2011 Social Network Challenge are evidence that this issue is as relevant today as it has ever been. While acknowledging the importance and prevalence of leakage in both synthetic competitions and real-life data mining projects, existing literature has largely left this idea unexplored. What little has been said turns out not to be broad enough to cover more complex cases of leakage, such as those where the classical i.i.d. assumption is violated, that have been recently documented. In our new approach, these cases and others are explained by explicitly defining modeling goals and analyzing the broader framework of the data mining problem. The resulting definition enables us to derive general methodology for dealing with the issue. We show that it is possible to avoid leakage with a simple specific approach to data management followed by what we call a learn-predict separation, and present several ways of detecting leakage when the modeler has no control over how the data have been collected.},
  isbn = {978-1-4503-0813-7},
  keywords = {Information privacy,leakage},
  series = {{{KDD}} '11},
  venue = {San Diego, California, USA}
}

@article{kent1955MachineLiteratureSearching,
  title = {Machine literature searching {{VIII}}. {{Operational}} criteria for designing information retrieval systems},
  author = {Kent, Allen and Berry, Madeline M. and Luehrs, Fred U. Jr. and Perry, J. W.},
  date = {1955},
  journaltitle = {American Documentation},
  volume = {6},
  pages = {93--101},
  issn = {1936-6108},
  doi = {10.1002/asi.5090060209},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.5090060209},
  urldate = {2019-11-30},
  langid = {english},
  number = {2}
}

@online{kuehlkamp2017GenderFromIrisGenderFromMascara,
  title = {Gender-{{From}}-{{Iris}} or {{Gender}}-{{From}}-{{Mascara}}?},
  author = {Kuehlkamp, Andrey and Becker, Benedict and Bowyer, Kevin},
  date = {2017-02-04},
  url = {http://arxiv.org/abs/1702.01304},
  urldate = {2019-03-14},
  abstract = {Predicting a person's gender based on the iris texture has been explored by several researchers. This paper considers several dimensions of experimental work on this problem, including person-disjoint train and test, and the effect of cosmetics on eyelash occlusion and imperfect segmentation. We also consider the use of multi-layer perceptron and convolutional neural networks as classifiers, comparing the use of data-driven and hand-crafted features. Our results suggest that the gender-from-iris problem is more difficult than has so far been appreciated. Estimating accuracy using a mean of N person-disjoint train and test partitions, and considering the effect of makeup - a combination of experimental conditions not present in any previous work - we find a much weaker ability to predict gender-from-iris texture than has been suggested in previous work.},
  archiveprefix = {arXiv},
  eprint = {1702.01304},
  eprinttype = {arxiv},
  keywords = {computer vision,counterexample},
  primaryclass = {cs}
}

@inproceedings{lafferty2001ConditionalRandomFields,
  title = {Conditional {{Random Fields}}: {{Probabilistic Models}} for {{Segmenting}} and {{Labeling Sequence Data}}},
  shorttitle = {Conditional {{Random Fields}}},
  author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando},
  date = {2001},
  pages = {282--289},
  publisher = {{Morgan Kaufmann}},
  location = {{San Francisco, CA, USA}},
  url = {http://dl.acm.org/citation.cfm?id=645530.655813},
  urldate = {2019-04-30},
  eventtitle = {18th {{International Conference}} on {{Machine Learning}}},
  isbn = {978-1-55860-778-1},
  keywords = {\#nosource},
  series = {{{ICML}} '01}
}

@online{lehman2018SurprisingCreativityDigital,
  title = {The {{Surprising Creativity}} of {{Digital Evolution}}: {{A Collection}} of {{Anecdotes}} from the {{Evolutionary Computation}} and {{Artificial Life Research Communities}}},
  shorttitle = {The {{Surprising Creativity}} of {{Digital Evolution}}},
  author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Frénoy, Antoine and Gagné, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, François and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinski, Jason},
  date = {2018-03-09},
  url = {http://arxiv.org/abs/1803.03453},
  urldate = {2019-02-21},
  abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
  archiveprefix = {arXiv},
  eprint = {1803.03453},
  eprinttype = {arxiv},
  keywords = {artificial intelligence,counterexample,evolutionary algorithm,failure,machine learning},
  primaryclass = {cs}
}

@article{lu2010ZipfLawLeads,
  title = {Zipf's {{Law Leads}} to {{Heaps}}' {{Law}}: {{Analyzing Their Relation}} in {{Finite}}-{{Size Systems}}},
  shorttitle = {Zipf's {{Law Leads}} to {{Heaps}}' {{Law}}},
  author = {Lü, Linyuan and Zhang, Zi-Ke and Zhou, Tao},
  date = {2010-12-02},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {5},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0014139},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996287/},
  urldate = {2019-08-12},
  abstract = {Background Zipf's law and Heaps' law are observed in disparate complex systems. Of particular interests, these two laws often appear together. Many theoretical models and analyses are performed to understand their co-occurrence in real systems, but it still lacks a clear picture about their relation. Methodology/Principal Findings We show that the Heaps' law can be considered as a derivative phenomenon if the system obeys the Zipf's law. Furthermore, we refine the known approximate solution of the Heaps' exponent provided the Zipf's exponent. We show that the approximate solution is indeed an asymptotic solution for infinite systems, while in the finite-size system the Heaps' exponent is sensitive to the system size. Extensive empirical analysis on tens of disparate systems demonstrates that our refined results can better capture the relation between the Zipf's and Heaps' exponents. Conclusions/Significance The present analysis provides a clear picture about the relation between the Zipf's law and Heaps' law without the help of any specific stochastic model, namely the Heaps' law is indeed a derivative phenomenon from the Zipf's law. The presented numerical method gives considerably better estimation of the Heaps' exponent given the Zipf's exponent and the system size. Our analysis provides some insights and implications of real complex systems. For example, one can naturally obtained a better explanation of the accelerated growth of scale-free networks.},
  eprint = {21152034},
  eprinttype = {pmid},
  number = {12},
  pmcid = {PMC2996287}
}

@inproceedings{muzerelle2013ANCORPremierCorpus,
  title = {{{ANCOR}}, premier corpus de français parlé d'envergure annoté en coréférence et distribué librement},
  booktitle = {Actes de la 20ème conférence sur le {{Traitement Automatique}} des {{Langues Naturelles}}},
  author = {Muzerelle, Judith and Lefeuvre, Anaïs and Antoine, Jean-Yves and Schang, Emmanuel and Maurel, Denis and Villaneau, Jeanne and Eshkol, Iris},
  date = {2013-06},
  pages = {555--563},
  publisher = {{Association pour le Traitement Automatique des Langues}},
  location = {{Les Sable d'Olonne, France}},
  url = {https://hal.archives-ouvertes.fr/hal-01016562},
  urldate = {2017-01-31},
  eventtitle = {{{TALN}}'2013},
  keywords = {anaphora,anaphore,annotation,copora/ANCOR,coreference,coréférence,corpora,corpora/ESLO,Corpus,dialogue,parole conversationnelle}
}

@software{neubig2019TutorialProgrammingNatural,
  title = {A {{Tutorial}} about {{Programming}} for {{Natural Language Processing}}},
  shorttitle = {A {{Tutorial}} about {{Programming}} for {{Natural Language Processing}}},
  author = {Neubig, Graham},
  date = {2019-04-27T17:18:33Z},
  origdate = {2014-05-31T02:12:42Z},
  url = {https://github.com/neubig/nlptutorial},
  urldate = {2019-04-30},
  howpublished = {online},
  keywords = {\#nosource}
}

@unpublished{preux2011FouilleDonneesNotes,
  title = {Fouille de données - {{Notes}} de cours},
  author = {Preux, Philippe},
  date = {2011-05-26},
  url = {http://www.grappa.univ-lille3.fr/~ppreux/Documents/notes-de-cours-de-fouille-de-donnees.pdf},
  urldate = {2018-01-13},
  howpublished = {online},
  keywords = {\#nosource}
}

@incollection{ramshaw1999TextChunkingUsing,
  title = {Text {{Chunking Using Transformation}}-{{Based Learning}}},
  booktitle = {Natural {{Language Processing Using Very Large Corpora}}},
  author = {Ramshaw, Lance A. and Marcus, Mitchell P.},
  editor = {Armstrong, Susan and Church, Kenneth and Isabelle, Pierre and Manzi, Sandra and Tzoukermann, Evelyne and Yarowsky, David},
  date = {1999},
  pages = {157--176},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-017-2390-9_10},
  url = {https://doi.org/10.1007/978-94-017-2390-9_10},
  urldate = {2020-02-02},
  abstract = {Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive “baseNP” chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93\% for baseNP chunks (trained on 950K words) and 88\% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach.},
  isbn = {978-94-017-2390-9},
  keywords = {Apply Natural Language Processing,Lexical Rule,Noun Phrase,Prepositional Phrase,Rule Template},
  langid = {english},
  series = {Text, {{Speech}} and {{Language Technology}}}
}

@unpublished{sasaki2007TruthFmeasure,
  title = {The truth of the {{F}}-measure},
  author = {Sasaki, Yutaka},
  date = {2007-11-26},
  url = {https://www.cs.odu.edu/~mukka/cs795sum09dm/Lecturenotes/Day3/F-measure-YS-26Oct07.pdf},
  abstract = {It has been past more than 15 years since the F-measure was first introduced to evaluation tasks of information extraction technology at the Fourth Message Understanding Conference (MUC-4) in 1992. Recently, sometimes I see some confusion with the definition of the Fmeasure, which seems to be triggered by lack of background knowledge about how the F-measure was derived. Since I was not involved in the process of the introduction or device of the F-measure, I might not be the best person to explain this but I hope this note would be a little help for those who are wondering what the F-measure really is. This introduction is devoted to provide brief but sufficient information on the F-measure.},
  howpublished = {Lecture notes},
  langid = {english},
  pagetotal = {5},
  type = {Lecture notes}
}

@inproceedings{sims1994EvolvingVirtualCreatures,
  title = {Evolving {{Virtual Creatures}}},
  booktitle = {Proceedings of the 21st {{Annual Conference}} on {{Computer Graphics}} and {{Interactive Techniques}}},
  author = {Sims, Karl},
  date = {1994},
  pages = {15--22},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/192161.192167},
  url = {http://doi.acm.org/10.1145/192161.192167},
  urldate = {2019-02-21},
  abstract = {This paper describes a novel system for creating virtual creatures that move and behave in simulated three-dimensional physical worlds. The morphologies of creatures and the neural systems for controlling their muscle forces are both generated automatically using genetic algorithms. Different fitness evaluation functions are used to direct simulated evolutions towards specific behaviors such as swimming, walking, jumping, and following.A genetic language is presented that uses nodes and connections as its primitive elements to represent directed graphs, which are used to describe both the morphology and the neural circuitry of these creatures. This genetic language defines a hyperspace containing an indefinite number of possible creatures with behaviors, and when it is searched using optimization techniques, a variety of successful and interesting locomotion strategies emerge, some of which would be difficult to invent or built by design.},
  isbn = {978-0-89791-667-7},
  keywords = {artificial intelligence,evolutionary algorithm},
  series = {{{SIGGRAPH}} '94}
}

@inproceedings{slonim2014ClaimsDemandInitial,
  title = {Claims on demand - an initial demonstration of a system for automatic detection and polarity identification of context dependent claims in massive corpora},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Computational Linguistics}}},
  author = {Slonim, Noam and Aharoni, Ehud and Perez, Carlos Alzate and Bar-Haim, Roy and Bilu, Yonatan and Dankin, Lena and Eiron, Iris and Hershcovich, Daniel and Hummel, Shay and Khapra, Mitesh M. and Lavee, Tamar and Levy, Ran and Matchen, Paul and Polnarov, Anatoly and Raykar, Vikas C. and Rinott, Ruty and Saha, Amrita and Zwerdling, Naama and Konopnicki, David and Gutfreund, Dan},
  editor = {Tounsi, Lamia and Rak, Rafal},
  date = {2014},
  pages = {6--9},
  publisher = {{Association for Computational Linguistics}},
  url = {http://aclweb.org/anthology/C/C14/C14-2002.pdf},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {http://dblp.org/rec/bib/conf/coling/SlonimAABBDEHHKLLMPRRSZKG14},
  eventtitle = {{{COLING}} 2014},
  isbn = {978-1-941643-27-3},
  venue = {Baile Átha Cliath, Éire}
}

@book{sorensen1948MethodEstablishingGroups,
  title = {A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on {{Danish}} commons.},
  author = {Sørensen, Thorvald Julius},
  date = {1948},
  publisher = {{E. Munksgaard}},
  location = {{København}},
  annotation = {OCLC: 4713331},
  langid = {english}
}

@online{SpecificationGamingExamples,
  title = {Specification gaming examples in {{AI}} - master list},
  url = {https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml},
  urldate = {2019-02-21},
  keywords = {artificial intelligence,counterexample,evolutionary algorithm,failure,machine learning}
}

@unpublished{tellier2017IntroductionFouilleTextes,
  title = {Introduction à la fouille de textes},
  author = {Tellier, Isabelle},
  date = {2017-11-10},
  url = {http://www.lattice.cnrs.fr/sites/itellier/poly_fouille_textes/fouille-textes.pdf},
  urldate = {2018-01-16},
  howpublished = {online},
  keywords = {\#nosource},
  langid = {french},
  type = {Lecture notes}
}

@thesis{teufel1999ArgumentativeZoningInformation,
  title = {Argumentative {{Zoning}}: {{Information Extraction}} from {{Scientific Text}}},
  author = {Teufel, Simone},
  date = {1999},
  location = {{Edinburgh, Scotland}},
  url = {http://www.cl.cam.ac.uk/users/sht25/az.html},
  keywords = {\#nosource},
  pagetotal = {284+}
}

@book{vanrijsbergen1979InformationRetrieval,
  title = {Information {{Retrieval}}},
  author = {van Rijsbergen, Cornelis Joost},
  date = {1979-01-01},
  publisher = {{Butterworth-Heinemann}},
  url = {http://dl.acm.org/citation.cfm?id=539927},
  urldate = {2019-11-30},
  isbn = {978-0-408-70929-3},
  options = {useprefix=true}
}

@inproceedings{vogt2004MinimumCostEmergence,
  title = {Minimum cost and the emergence of the {{Zipf}}-{{Mandelbrot}} law},
  booktitle = {Artificial {{Life IX}}: {{Proceedings}} of the {{Ninth International Conference}} on the {{Simulation}} and {{Synthesis}} of {{Living Systems}}},
  author = {Vogt, Paul},
  date = {2004},
  keywords = {\#nosource},
  organization = {{MIT Press}}
}

@book{zipf1949HumanBehaviorPrinciple,
  title = {Human behavior and the principle of least effort: {{An}} introduction to human ecology},
  author = {Zipf, George Kingsley},
  date = {1949},
  publisher = {{Addison-Wesley Press}},
  location = {{Oxford, England}},
  keywords = {\#nosource}
}


